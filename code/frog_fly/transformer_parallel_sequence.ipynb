{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 PPO + Transformer-Based Policy Network\n",
    "\n",
    "To incorporate temporal context into PPO, we replace the MLP policy with a Transformer policy.  \n",
    "Instead of conditioning only on the current state, the agent receives a short history of experience and attends to relevant past transitions.\n",
    "\n",
    "---\n",
    "\n",
    "### Sequential Input Representation\n",
    "\n",
    "At timestep \\(t\\), we construct a window of recent experience:\n",
    "\n",
    "$$\n",
    "\\mathcal{E}_t = \\big[(s_{t-T+1}, a_{t-T+1}), \\dots, (s_{t-1},a_{t-1}), (s_t,a_t)\\big]\n",
    "$$\n",
    "\n",
    "Each element is embedded using a small MLP:\n",
    "\n",
    "$$\n",
    "x_i = \\mathrm{MLP}_{\\text{embed}}([s_i, a_i])\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Temporal Positional Encoding\n",
    "\n",
    "Because self-attention is order-invariant, we inject temporal indices using sinusoidal encodings:\n",
    "\n",
    "$$\n",
    "PE(i) = [\\sin(\\omega_k i), \\cos(\\omega_k i)]_{k=1}^{F}\n",
    "$$\n",
    "\n",
    "The input token for the Transformer becomes:\n",
    "\n",
    "$$\n",
    "z_i = x_i \\;\\Vert\\; PE(i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Transformer Encoder Block\n",
    "\n",
    "A Transformer layer processes the sequence by attending over past and recent experience:\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\mathrm{LN}\\Big(H^{(l)} +\n",
    "\\mathrm{MHA}(H^{(l)}, H^{(l)}, H^{(l)})\\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\mathrm{LN}\\Big(H^{(l)} + \\mathrm{FFN}(H^{(l)})\\Big)\n",
    "$$\n",
    "\n",
    "The final hidden state \\(H_T\\) serves as a context summary for decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### Policy & Value Outputs\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(a_t|\\mathcal{E}_t) =\n",
    "\\mathrm{Softmax}(\\mathrm{MLP}_{\\text{policy}}(H_T))\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_\\phi(\\mathcal{E}_t) =\n",
    "\\mathrm{MLP}_{\\text{value}}(H_T)\n",
    "$$\n",
    "\n",
    "This allows PPO to act based on *recent experience*, not just a single observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (needed for 3D)\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_3d(pos, num_freqs=4, max_freq=10.0):\n",
    "    \"\"\"\n",
    "    pos: np.array shape (3,) with (x, y, z) in [-space_size, space_size].\n",
    "    Returns: encoding vector of shape (3 * 2 * num_freqs,)\n",
    "    \"\"\"\n",
    "    pos = np.asarray(pos, dtype=np.float32)\n",
    "    assert pos.shape == (3,)\n",
    "\n",
    "    freqs = np.linspace(1.0, max_freq, num_freqs)\n",
    "    enc = []\n",
    "    for coord in pos:            # x, y, z\n",
    "        for f in freqs:\n",
    "            enc.append(np.sin(f * coord))\n",
    "            enc.append(np.cos(f * coord))\n",
    "    return np.array(enc, dtype=np.float32)\n",
    "\n",
    "\n",
    "class FrogFly3DEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    3D continuous environment:\n",
    "    - Frog (agent) moves to catch a moving fly.\n",
    "    - Observations: positions + relative vector + positional encodings.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            config = {}\n",
    "\n",
    "        self.space_size = float(config.get(\"space_size\", 1.0))\n",
    "        self.step_size = float(config.get(\"step_size\", 0.1))\n",
    "        self.fly_speed = float(config.get(\"fly_speed\", 0.05))\n",
    "        self.catch_radius = float(config.get(\"catch_radius\", 0.15))\n",
    "        self.max_steps = int(config.get(\"max_steps\", 200))\n",
    "        self.num_freqs = int(config.get(\"num_freqs\", 4))\n",
    "        self.use_positional_encodings = bool(\n",
    "            config.get(\"use_positional_encodings\", True)\n",
    "        )\n",
    "\n",
    "        # Action = 3D movement in [-1, 1]^3\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(3,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation components\n",
    "        # frog_pos (3) + fly_pos (3) + rel (3) = 9\n",
    "        base_dim = 9\n",
    "        if self.use_positional_encodings:\n",
    "            pe_dim = 3 * 2 * self.num_freqs  # per position\n",
    "            obs_dim = base_dim + 2 * pe_dim\n",
    "        else:\n",
    "            obs_dim = base_dim\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-5.0, high=5.0, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.frog_pos = None\n",
    "        self.fly_pos = None\n",
    "        self.steps = 0\n",
    "        self._value_out = None\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "\n",
    "    def _sample_position(self):\n",
    "        return np.random.uniform(\n",
    "            low=-self.space_size,\n",
    "            high=self.space_size,\n",
    "            size=(3,),\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def _clip_position(self, pos):\n",
    "        return np.clip(pos, -self.space_size, self.space_size).astype(np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        rel = self.fly_pos - self.frog_pos\n",
    "        parts = [self.frog_pos, self.fly_pos, rel]\n",
    "\n",
    "        if self.use_positional_encodings:\n",
    "            frog_pe = positional_encoding_3d(self.frog_pos, self.num_freqs)\n",
    "            fly_pe = positional_encoding_3d(self.fly_pos, self.num_freqs)\n",
    "            parts.extend([frog_pe, fly_pe])\n",
    "\n",
    "        return np.concatenate(parts, axis=0).astype(np.float32)\n",
    "\n",
    "    def _get_distance(self):\n",
    "        return float(np.linalg.norm(self.fly_pos - self.frog_pos))\n",
    "\n",
    "    # ---------- Gymnasium API ----------\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.frog_pos = self._sample_position()\n",
    "        self.fly_pos = self._sample_position()\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        action = np.asarray(action, dtype=np.float32)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        # Frog moves\n",
    "        self.frog_pos = self.frog_pos + self.step_size * action\n",
    "        self.frog_pos = self._clip_position(self.frog_pos)\n",
    "\n",
    "        # Fly moves randomly\n",
    "        fly_dir = np.random.normal(size=(3,)).astype(np.float32)\n",
    "        fly_dir /= np.linalg.norm(fly_dir) + 1e-8\n",
    "        self.fly_pos = self.fly_pos + self.fly_speed * fly_dir\n",
    "        self.fly_pos = self._clip_position(self.fly_pos)\n",
    "\n",
    "        dist = self._get_distance()\n",
    "        caught = dist < self.catch_radius\n",
    "\n",
    "        # Reward shaping: closer is better + bonus for catch\n",
    "        reward = -dist\n",
    "        if caught:\n",
    "            reward += 10.0\n",
    "\n",
    "        terminated = caught\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {\"distance\": dist, \"caught\": caught}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"Step {self.steps} | \"\n",
    "            f\"Frog: {self.frog_pos} | Fly: {self.fly_pos} | \"\n",
    "            f\"Dist: {self._get_distance():.3f}\"\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "# K-step observation history wrapper (if you don't already have this)\n",
    "class ObsHistoryWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, K=8):\n",
    "        super().__init__(env)\n",
    "        self.K = K\n",
    "        obs_shape = env.observation_space.shape  # (D,)\n",
    "        self.history = np.zeros((K,) + obs_shape, dtype=np.float32)\n",
    "\n",
    "        low  = np.repeat(env.observation_space.low[None, :],  K, axis=0)\n",
    "        high = np.repeat(env.observation_space.high[None, :], K, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.history[:] = obs\n",
    "        return self.history.copy(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.history = np.roll(self.history, shift=-1, axis=0)\n",
    "        self.history[-1] = obs\n",
    "        return self.history.copy(), reward, terminated, truncated, info\n",
    "\n",
    "# Factory for RLlib\n",
    "def make_frog_hist_env(env_config):\n",
    "    base_env = FrogFly3DEnv(env_config)\n",
    "    return ObsHistoryWrapper(base_env, K=8)\n",
    "\n",
    "# Register under a name RLlib understands\n",
    "register_env(\"frog_hist_env\", make_frog_hist_env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 02:22:30,652\tINFO worker.py:2023 -- Started a local Ray instance.\n",
      "/home/platinumfish/miniconda3/envs/rl-project/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "class FrogTransformerModel(TorchModelV2, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        # Expect obs shape = (K, obs_dim_single)\n",
    "        assert len(obs_space.shape) == 2, f\"Expected (K, D), got {obs_space.shape}\"\n",
    "        self.seq_len    = obs_space.shape[0]  # K (time)\n",
    "        self.feature_dim = obs_space.shape[1] # D (per-timestep features)\n",
    "\n",
    "        cmc = model_config.get(\"custom_model_config\", {}) or {}\n",
    "\n",
    "        def get_hp(key, default):\n",
    "            return kwargs.get(key, cmc.get(key, default))\n",
    "\n",
    "        d_model    = get_hp(\"d_model\", 64)\n",
    "        nhead      = get_hp(\"nhead\", 4)\n",
    "        num_layers = get_hp(\"num_layers\", 1)\n",
    "        dim_ff     = get_hp(\"dim_feedforward\", 128)\n",
    "        dropout    = get_hp(\"dropout\", 0.1)\n",
    "\n",
    "        # Embedding per timestep (map R^{D} -> R^{d_model})\n",
    "        self.embed = nn.Linear(self.feature_dim, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_outputs),\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "        self._features = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Get obs as float tensor, whatever its shape is\n",
    "        x = input_dict[\"obs\"].float()       # shape: (B, ...) with possibly >2 dims\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Flatten all non-batch dims into a single feature dimension\n",
    "        x = x.view(B, -1)                   # shape: (B, F_flat)\n",
    "        F = x.shape[1]\n",
    "\n",
    "        # --- Infer how to interpret the obs as (B, K, D) sequence ---\n",
    "        # Case 1: env gives a single feature vector of length feature_dim → treat as K=1\n",
    "        if F == self.feature_dim:\n",
    "            K = 1\n",
    "        # Case 2: env gives a flattened sequence of length seq_len * feature_dim\n",
    "        elif F == self.seq_len * self.feature_dim:\n",
    "            K = self.seq_len\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot reshape obs of size {F} into \"\n",
    "                f\"(B, K, feature_dim={self.feature_dim}) with seq_len={self.seq_len}.\"\n",
    "            )\n",
    "\n",
    "        # Reshape to (B, K, D)\n",
    "        x = x.view(B, K, self.feature_dim)\n",
    "\n",
    "        # --- Standard Transformer forward ---\n",
    "        # (B, K, D) → (B, K, d_model)\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # Optional: positional encodings, if you defined self.pos_embed\n",
    "        if hasattr(self, \"pos_embed\"):\n",
    "            # assume pos_embed has shape (1, max_K, d_model)\n",
    "            x = x + self.pos_embed[:, :K, :]\n",
    "\n",
    "        # TransformerEncoder expects (K, B, d_model)\n",
    "        x = x.transpose(0, 1)          # (K, B, d_model)\n",
    "        x = self.transformer(x)        # (K, B, d_model)\n",
    "        x = x.transpose(0, 1)          # (B, K, d_model)\n",
    "\n",
    "        # Pooling: take last token\n",
    "        token = x[:, -1, :]            # (B, d_model)\n",
    "\n",
    "        # Policy and value heads\n",
    "        logits = self.policy_head(token)     # (B, num_actions)\n",
    "        value  = self.value_head(token)      # (B, 1)\n",
    "\n",
    "        # RLlib expects value via this attribute\n",
    "        self._value_out = value.squeeze(-1)  # (B,)\n",
    "\n",
    "        # Return logits + empty RNN state list\n",
    "        return logits, []\n",
    "\n",
    "    def value_function(self):\n",
    "        # RLlib calls this after forward() to get V(s).\n",
    "        if self._value_out is None:\n",
    "            raise ValueError(\"value_function() called before forward().\")\n",
    "        return self._value_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ModelCatalog.register_custom_model(\"frog_transformer_policy\", FrogTransformerModel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frog_env(env_config):\n",
    "    base_env = FrogFly3DEnv(env_config)\n",
    "    return ObsHistoryWrapper(base_env, K=8)  # e.g., last 8 obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 02:22:32,094\tWARNING 3989743005.py:61 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "/home/platinumfish/miniconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/platinumfish/miniconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/platinumfish/miniconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/platinumfish/miniconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-12-02 02:22:32,210 E 2245261 2245261] core_worker.cc:2223: Actor with class name: 'RolloutWorker' and ID: '9c40c96d07983be2e6b3189a01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-12-02 02:22:39,303\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo built OK with grad clipping\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_config = {\n",
    "    \"space_size\": 1.0,\n",
    "    \"step_size\": 0.1,\n",
    "    \"fly_speed\": 0.05,\n",
    "    \"catch_radius\": 0.15,\n",
    "    \"max_steps\": 200,\n",
    "    \"num_freqs\": 4,\n",
    "    \"use_positional_encodings\": True,\n",
    "}\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"frog_hist_env\",    # registered history-wrapped env\n",
    "        env_config=env_config,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False,\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=1,\n",
    "        num_envs_per_env_runner=4,\n",
    "        create_env_on_local_worker=True,\n",
    "    )\n",
    "    .resources(num_gpus=0)\n",
    "    .training(\n",
    "        model={\n",
    "            \"custom_model\": \"frog_transformer_policy\",\n",
    "            \"custom_model_config\": {\n",
    "                \"d_model\": 64,\n",
    "                \"nhead\": 4,\n",
    "                \"num_layers\": 1,\n",
    "                \"dim_feedforward\": 128,\n",
    "                \"dropout\": 0.1,\n",
    "                \"entropy_coeff\": 0.01\n",
    "            },\n",
    "        },\n",
    "        gamma=0.99,\n",
    "        lr=1e-4,          # a bit conservative to help with stability\n",
    "        grad_clip=0.5,    # gradient clipping threshold\n",
    "        grad_clip_by=\"global_norm\",  # (this is the default but being explicit)\n",
    "        train_batch_size=2000,\n",
    "        num_epochs=10,\n",
    "        minibatch_size=256,\n",
    "        clip_param=0.2,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_duration=20,              # 20 eval episodes instead of 10\n",
    "        evaluation_duration_unit=\"episodes\", # interpret that as episodes\n",
    "        # optional, but good practice:\n",
    "        evaluation_config={\n",
    "            \"explore\": False,               # greedy eval\n",
    "        },\n",
    "    ) \n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(\"Algo built OK with grad clipping\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 02:22:41,618\tWARNING train_ops.py:114 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['custom_metrics', 'episode_media', 'info', 'env_runners', 'num_healthy_workers', 'actor_manager_num_outstanding_async_reqs', 'num_remote_worker_restarts', 'num_agent_steps_sampled', 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'num_env_steps_sampled_throughput_per_sec', 'num_env_steps_trained_throughput_per_sec', 'timesteps_total', 'num_env_steps_sampled_lifetime', 'num_agent_steps_sampled_lifetime', 'num_steps_trained_this_iter', 'agent_timesteps_total', 'timers', 'counters', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore', 'perf'])\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(0.19999999999999993), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.968435178484235), 'policy_loss': np.float64(0.07700169272720814), 'vf_loss': np.float64(9.83353978565761), 'vf_explained_var': np.float64(-0.00037115727152143207), 'kl': np.float64(0.2894685424864292), 'entropy': np.float64(4.809104626519339), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(35.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'env_runners': {'episode_reward_max': -9.359816014766693, 'episode_reward_min': -419.73174798488617, 'episode_reward_mean': np.float64(-268.56415336206555), 'episode_len_mean': np.float64(167.9), 'episode_media': {}, 'episodes_timesteps_total': 1679, 'policy_reward_min': {'default_policy': np.float64(-419.73174798488617)}, 'policy_reward_max': {'default_policy': np.float64(-9.359816014766693)}, 'policy_reward_mean': {'default_policy': np.float64(-268.56415336206555)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-58.878856636583805, -275.04840920865536, -294.4818652868271, -386.4247170686722, -346.00027418136597, -346.87547302246094, -419.73174798488617, -174.2498907893896, -9.359816014766693, -374.59048342704773], 'episode_lengths': [79, 166, 200, 200, 200, 200, 200, 200, 34, 200], 'policy_default_policy_reward': [-58.878856636583805, -275.04840920865536, -294.4818652868271, -386.4247170686722, -346.00027418136597, -346.87547302246094, -419.73174798488617, -174.2498907893896, -9.359816014766693, -374.59048342704773]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.180855813854469), 'mean_inference_ms': np.float64(2.043960099210758), 'mean_action_processing_ms': np.float64(0.49799740195512293), 'mean_env_wait_ms': np.float64(0.7595639980719714), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.00820159912109375), 'StateBufferConnector_ms': np.float64(0.006701946258544922), 'ViewRequirementAgentConnector_ms': np.float64(0.10672330856323242)}, 'num_episodes': 10, 'episode_return_max': -9.359816014766693, 'episode_return_min': -419.73174798488617, 'episode_return_mean': np.float64(-268.56415336206555), 'episodes_this_iter': 10}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 293.82705965090634, 'num_env_steps_trained_throughput_per_sec': 293.82705965090634, 'timesteps_total': 2000, 'num_env_steps_sampled_lifetime': 2000, 'num_agent_steps_sampled_lifetime': 2000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 6806.745, 'restore_workers_time_ms': 0.022, 'training_step_time_ms': 6806.663, 'sample_time_ms': 2303.854, 'load_time_ms': 0.543, 'load_throughput': 3685680.141, 'learn_time_ms': 4493.359, 'learn_throughput': 445.101, 'synch_weights_time_ms': 7.019}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-12-02_02-22-46', 'timestamp': 1764660166, 'time_this_iter_s': 6.811600208282471, 'time_total_s': 6.811600208282471, 'pid': 2245261, 'hostname': 'platinumfish-Aspire', 'node_ip': '192.168.86.52', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'frog_hist_env', 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1, 'entropy_coeff': 0.01}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x784be534d360>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 6.811600208282471, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(53.86999999999999), 'ram_util_percent': np.float64(70.75)}}\n"
     ]
    }
   ],
   "source": [
    "res = algo.train()\n",
    "print(res.keys())\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: reward_mean=-280.23, len_mean=183.9\n",
      "Iter 2: reward_mean=-298.76, len_mean=189.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-02 02:22:59,547 E 2248215 2248215] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-12-02 02:23:00,613 E 2248330 2248330] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(RolloutWorker pid=2248382)\u001b[0m [2025-12-02 02:23:01,699 E 2248382 2248495] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-02 02:23:02,043 E 2245261 2248375] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3: reward_mean=-297.60, len_mean=192.0\n",
      "Iter 4: reward_mean=-278.83, len_mean=188.0\n",
      "Iter 5: reward_mean=-274.44, len_mean=188.1\n",
      "Iter 6: reward_mean=-275.81, len_mean=189.9\n",
      "Iter 7: reward_mean=-268.98, len_mean=187.6\n",
      "Iter 8: reward_mean=-270.79, len_mean=188.9\n",
      "Iter 9: reward_mean=-272.26, len_mean=191.3\n",
      "Iter 10: reward_mean=-267.32, len_mean=192.2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "\n",
    "    er = result[\"env_runners\"]  # shorthand\n",
    "\n",
    "    reward_mean = er[\"episode_return_mean\"]\n",
    "    len_mean    = er[\"episode_len_mean\"]\n",
    "\n",
    "    print(\n",
    "        f\"Iter {i+1}: \"\n",
    "        f\"reward_mean={float(reward_mean):.2f}, \"\n",
    "        f\"len_mean={float(len_mean):.1f}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metrics(result):\n",
    "    \"\"\"Extract mean reward and len from an RLlib result dict (env_runners layout).\"\"\"\n",
    "    er = result.get(\"env_runners\", {})\n",
    "    reward_mean = float(er.get(\"episode_return_mean\", float(\"nan\")))\n",
    "    len_mean    = float(er.get(\"episode_len_mean\", float(\"nan\")))\n",
    "    return reward_mean, len_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eval_env = FrogFly3DEnv(env_config)  # reuse your env_config\n",
    "\n",
    "\n",
    "def evaluate_policy(algo, env, num_episodes=5, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate a single-agent policy using the legacy RLlib API.\n",
    "\n",
    "    Uses algo.compute_single_action(obs, ...) which is the right call\n",
    "    for this RLlib version (compute_actions is multi-agent only here).\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    lengths = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Single-agent action: returns (action, state_out, info)\n",
    "            action, _, _ = algo.compute_single_action(obs, explore=False)\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ep_ret += reward\n",
    "            steps += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        returns.append(ep_ret)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    return {\n",
    "        \"mean_return\": float(np.mean(returns)),\n",
    "        \"std_return\": float(np.std(returns)),\n",
    "        \"mean_length\": float(np.mean(lengths)),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter    1] train_reward= -260.40, train_len= 190.9\n",
      "[Iter    2] train_reward= -255.53, train_len= 189.5\n",
      "[Iter    3] train_reward= -256.08, train_len= 188.1\n",
      "[Iter    4] train_reward= -256.83, train_len= 188.5\n",
      "[Iter    5] train_reward= -257.20, train_len= 188.7\n",
      "[Iter    6] train_reward= -262.91, train_len= 190.2\n",
      "[Iter    7] train_reward= -261.13, train_len= 190.3\n",
      "[Iter    8] train_reward= -261.91, train_len= 188.8\n",
      "[Iter    9] train_reward= -259.03, train_len= 188.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 02:24:49,971\tWARNING 1093147107.py:24 -- DeprecationWarning: `compute_single_action` has been deprecated. `Algorithm.compute_single_action` should no longer be used. Get the RLModule instance through `Algorithm.get_module([module ID])`, then compute actions through `RLModule.forward_inference({'obs': [obs batch]})`. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter   10] train_reward= -254.61, train_len= 188.5 | eval_return= -237.28, eval_len= 200.0\n",
      "  New best eval_return=-237.28, saved to TrainingResult(checkpoint=Checkpoint(filesystem=local, path=frog_transformer_checkpoints), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(0.3000000000000001), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.4270535332816), 'policy_loss': np.float64(0.03399819646562849), 'vf_loss': np.float64(9.390232310976302), 'vf_explained_var': np.float64(-0.005456646851130894), 'kl': np.float64(0.009410177204491837), 'entropy': np.float64(5.0055889129638675), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(1435.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 42000, 'num_env_steps_trained': 42000, 'num_agent_steps_sampled': 42000, 'num_agent_steps_trained': 42000}, 'env_runners': {'episode_reward_max': 9.886653579771519, 'episode_reward_min': -429.19836843013763, 'episode_reward_mean': np.float64(-254.61435630068183), 'episode_len_mean': np.float64(188.46), 'episode_media': {}, 'episodes_timesteps_total': 18846, 'policy_reward_min': {'default_policy': np.float64(-429.19836843013763)}, 'policy_reward_max': {'default_policy': np.float64(9.886653579771519)}, 'policy_reward_mean': {'default_policy': np.float64(-254.61435630068183)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-265.6648522615433, -34.55600714683533, -291.6092568039894, -280.2931792140007, -291.8879920244217, -364.97331511974335, -307.8879844248295, -201.6230280995369, -272.07990366220474, -363.79472827911377, -245.28545880317688, -346.3788299560547, -429.19836843013763, -34.16293589770794, -271.6242209672928, -337.4761841893196, -184.37534092366695, -259.40708154439926, -24.305643379688263, -305.51434594392776, -384.47448801994324, -265.5310717523098, -301.2649463415146, -135.9103593081236, -249.24795508384705, -370.42250072956085, 9.886653579771519, -296.1993451118469, -340.3945071697235, -225.9259530901909, -268.3042577803135, -263.4572058916092, -263.3128154873848, -333.25661277770996, -259.52569061517715, -180.92850311100483, -192.7385098040104, -230.02201962471008, -335.2931874990463, -314.1677148938179, -288.0659220814705, -393.77314949035645, -64.99838778376579, -217.92939707636833, -287.0962447524071, -307.39845329523087, -236.26036930084229, -413.6639093160629, -394.82621240615845, -256.0311616063118, -164.5598419904709, -341.8345687389374, -154.40797314047813, -200.02301961183548, -408.1070628166199, -310.44333279132843, -352.7141149044037, -212.12344443798065, -305.1945481300354, -297.9633505642414, -285.42897498607635, -373.0533809661865, -295.63964027166367, -69.645986109972, -181.9474459886551, -392.66901391744614, -95.54078848659992, -202.8373190164566, -326.459282040596, -389.82864928245544, -410.0239119529724, -209.24139918386936, -327.81273770332336, -306.6580767631531, -142.40379372239113, -236.73688781261444, -260.56150817871094, -42.357963502407074, -369.8910388946533, -217.9822717010975, -270.9886643886566, -112.3642184510827, -79.68081073462963, -198.85534381866455, -320.88775968551636, -236.81625118851662, -276.809588432312, -224.95010167360306, -217.41816943883896, -156.64015170931816, -135.0881988108158, -240.64440587162971, -324.61355620622635, -184.59133782982826, -275.92846524715424, -261.1781344115734, -208.92904910445213, -253.47021734714508, -239.51534974575043, -185.34167367219925], 'episode_lengths': [200, 73, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 64, 200, 200, 200, 200, 65, 200, 200, 200, 200, 188, 200, 200, 1, 200, 200, 200, 200, 200, 200, 200, 200, 153, 200, 200, 200, 200, 200, 200, 107, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 101, 200, 200, 157, 200, 200, 200, 200, 200, 200, 200, 176, 200, 200, 69, 200, 200, 200, 170, 122, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_default_policy_reward': [-265.6648522615433, -34.55600714683533, -291.6092568039894, -280.2931792140007, -291.8879920244217, -364.97331511974335, -307.8879844248295, -201.6230280995369, -272.07990366220474, -363.79472827911377, -245.28545880317688, -346.3788299560547, -429.19836843013763, -34.16293589770794, -271.6242209672928, -337.4761841893196, -184.37534092366695, -259.40708154439926, -24.305643379688263, -305.51434594392776, -384.47448801994324, -265.5310717523098, -301.2649463415146, -135.9103593081236, -249.24795508384705, -370.42250072956085, 9.886653579771519, -296.1993451118469, -340.3945071697235, -225.9259530901909, -268.3042577803135, -263.4572058916092, -263.3128154873848, -333.25661277770996, -259.52569061517715, -180.92850311100483, -192.7385098040104, -230.02201962471008, -335.2931874990463, -314.1677148938179, -288.0659220814705, -393.77314949035645, -64.99838778376579, -217.92939707636833, -287.0962447524071, -307.39845329523087, -236.26036930084229, -413.6639093160629, -394.82621240615845, -256.0311616063118, -164.5598419904709, -341.8345687389374, -154.40797314047813, -200.02301961183548, -408.1070628166199, -310.44333279132843, -352.7141149044037, -212.12344443798065, -305.1945481300354, -297.9633505642414, -285.42897498607635, -373.0533809661865, -295.63964027166367, -69.645986109972, -181.9474459886551, -392.66901391744614, -95.54078848659992, -202.8373190164566, -326.459282040596, -389.82864928245544, -410.0239119529724, -209.24139918386936, -327.81273770332336, -306.6580767631531, -142.40379372239113, -236.73688781261444, -260.56150817871094, -42.357963502407074, -369.8910388946533, -217.9822717010975, -270.9886643886566, -112.3642184510827, -79.68081073462963, -198.85534381866455, -320.88775968551636, -236.81625118851662, -276.809588432312, -224.95010167360306, -217.41816943883896, -156.64015170931816, -135.0881988108158, -240.64440587162971, -324.61355620622635, -184.59133782982826, -275.92846524715424, -261.1781344115734, -208.92904910445213, -253.47021734714508, -239.51534974575043, -185.34167367219925]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.0209035329068905), 'mean_inference_ms': np.float64(1.7779318664424122), 'mean_action_processing_ms': np.float64(0.4366070934704204), 'mean_env_wait_ms': np.float64(0.6735628786280645), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.004875898361206055), 'StateBufferConnector_ms': np.float64(0.003977298736572266), 'ViewRequirementAgentConnector_ms': np.float64(0.09388947486877441)}, 'num_episodes': 11, 'episode_return_max': 9.886653579771519, 'episode_return_min': -429.19836843013763, 'episode_return_mean': np.float64(-254.61435630068183), 'episodes_this_iter': 11}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 42000, 'num_agent_steps_trained': 42000, 'num_env_steps_sampled': 42000, 'num_env_steps_trained': 42000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 302.010021196053, 'num_env_steps_trained_throughput_per_sec': 302.010021196053, 'timesteps_total': 42000, 'num_env_steps_sampled_lifetime': 42000, 'num_agent_steps_sampled_lifetime': 42000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 42000, 'timers': {'training_iteration_time_ms': 6434.501, 'restore_workers_time_ms': 0.023, 'training_step_time_ms': 6434.435, 'sample_time_ms': 2056.03, 'load_time_ms': 0.591, 'load_throughput': 3384550.333, 'learn_time_ms': 4372.758, 'learn_throughput': 457.377, 'synch_weights_time_ms': 4.705}, 'counters': {'num_env_steps_sampled': 42000, 'num_env_steps_trained': 42000, 'num_agent_steps_sampled': 42000, 'num_agent_steps_trained': 42000}, 'done': False, 'training_iteration': 21, 'trial_id': 'default', 'date': '2025-12-02_02-24-49', 'timestamp': 1764660289, 'time_this_iter_s': 6.628499269485474, 'time_total_s': 130.2975800037384, 'pid': 2245261, 'hostname': 'platinumfish-Aspire', 'node_ip': '192.168.86.52', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'frog_hist_env', 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1, 'entropy_coeff': 0.01}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x784be534d360>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 130.2975800037384, 'iterations_since_restore': 21, 'perf': {'cpu_util_percent': np.float64(57.67999999999999), 'ram_util_percent': np.float64(69.64)}})\n",
      "[Iter   11] train_reward= -248.83, train_len= 186.5\n",
      "[Iter   12] train_reward= -243.89, train_len= 189.0\n",
      "[Iter   13] train_reward= -242.26, train_len= 189.2\n",
      "[Iter   14] train_reward= -243.17, train_len= 189.6\n",
      "[Iter   15] train_reward= -239.85, train_len= 188.2\n",
      "[Iter   16] train_reward= -232.33, train_len= 187.8\n",
      "[Iter   17] train_reward= -238.26, train_len= 189.2\n",
      "[Iter   18] train_reward= -243.28, train_len= 189.0\n",
      "[Iter   19] train_reward= -252.03, train_len= 186.8\n",
      "[Iter   20] train_reward= -257.54, train_len= 188.4 | eval_return= -268.04, eval_len= 200.0\n",
      "[Iter   21] train_reward= -261.26, train_len= 186.0\n",
      "[Iter   22] train_reward= -260.88, train_len= 184.0\n",
      "[Iter   23] train_reward= -258.15, train_len= 181.4\n",
      "[Iter   24] train_reward= -258.93, train_len= 180.8\n",
      "[Iter   25] train_reward= -255.64, train_len= 179.2\n",
      "[Iter   26] train_reward= -252.41, train_len= 179.2\n",
      "[Iter   27] train_reward= -249.68, train_len= 179.9\n",
      "[Iter   28] train_reward= -244.12, train_len= 180.6\n",
      "[Iter   29] train_reward= -245.26, train_len= 182.1\n",
      "[Iter   30] train_reward= -240.68, train_len= 178.3 | eval_return= -315.78, eval_len= 200.0\n",
      "[Iter   31] train_reward= -241.07, train_len= 180.2\n",
      "[Iter   32] train_reward= -248.02, train_len= 183.8\n",
      "[Iter   33] train_reward= -242.08, train_len= 184.4\n",
      "[Iter   34] train_reward= -246.21, train_len= 183.3\n",
      "[Iter   35] train_reward= -251.23, train_len= 184.3\n",
      "[Iter   36] train_reward= -259.16, train_len= 186.3\n",
      "[Iter   37] train_reward= -258.50, train_len= 186.8\n",
      "[Iter   38] train_reward= -262.23, train_len= 186.8\n",
      "[Iter   39] train_reward= -256.29, train_len= 186.2\n",
      "[Iter   40] train_reward= -260.58, train_len= 188.6 | eval_return= -431.62, eval_len= 200.0\n",
      "[Iter   41] train_reward= -263.45, train_len= 187.0\n",
      "[Iter   42] train_reward= -264.20, train_len= 185.8\n",
      "[Iter   43] train_reward= -266.47, train_len= 185.8\n",
      "[Iter   44] train_reward= -262.40, train_len= 185.8\n",
      "[Iter   45] train_reward= -258.22, train_len= 184.5\n",
      "[Iter   46] train_reward= -259.29, train_len= 184.2\n",
      "[Iter   47] train_reward= -263.74, train_len= 184.2\n",
      "[Iter   48] train_reward= -267.26, train_len= 185.0\n",
      "[Iter   49] train_reward= -269.16, train_len= 186.5\n",
      "[Iter   50] train_reward= -272.54, train_len= 189.4 | eval_return= -282.61, eval_len= 200.0\n",
      "[Iter   51] train_reward= -272.92, train_len= 191.9\n",
      "[Iter   52] train_reward= -278.78, train_len= 191.9\n",
      "[Iter   53] train_reward= -272.33, train_len= 190.0\n",
      "[Iter   54] train_reward= -279.31, train_len= 189.5\n",
      "[Iter   55] train_reward= -274.99, train_len= 189.5\n",
      "[Iter   56] train_reward= -274.52, train_len= 190.9\n",
      "[Iter   57] train_reward= -271.10, train_len= 192.4\n",
      "[Iter   58] train_reward= -267.03, train_len= 188.6\n",
      "[Iter   59] train_reward= -260.96, train_len= 187.0\n",
      "[Iter   60] train_reward= -259.01, train_len= 185.8 | eval_return= -317.67, eval_len= 200.0\n",
      "[Iter   61] train_reward= -258.64, train_len= 185.3\n",
      "[Iter   62] train_reward= -259.41, train_len= 187.0\n",
      "[Iter   63] train_reward= -256.44, train_len= 187.3\n",
      "[Iter   64] train_reward= -259.11, train_len= 189.3\n",
      "[Iter   65] train_reward= -251.91, train_len= 187.5\n",
      "[Iter   66] train_reward= -256.44, train_len= 188.6\n",
      "[Iter   67] train_reward= -256.69, train_len= 190.4\n",
      "[Iter   68] train_reward= -247.46, train_len= 189.2\n",
      "[Iter   69] train_reward= -249.49, train_len= 190.8\n",
      "[Iter   70] train_reward= -247.14, train_len= 192.6 | eval_return= -286.10, eval_len= 200.0\n",
      "[Iter   71] train_reward= -242.84, train_len= 193.0\n",
      "[Iter   72] train_reward= -238.39, train_len= 191.0\n",
      "[Iter   73] train_reward= -240.59, train_len= 191.7\n",
      "[Iter   74] train_reward= -238.54, train_len= 190.3\n",
      "[Iter   75] train_reward= -244.45, train_len= 192.2\n",
      "[Iter   76] train_reward= -241.06, train_len= 192.1\n",
      "[Iter   77] train_reward= -240.46, train_len= 191.0\n",
      "[Iter   78] train_reward= -251.63, train_len= 193.2\n",
      "[Iter   79] train_reward= -250.70, train_len= 192.1\n",
      "[Iter   80] train_reward= -254.34, train_len= 192.1 | eval_return= -371.92, eval_len= 200.0\n",
      "[Iter   81] train_reward= -253.99, train_len= 191.7\n",
      "[Iter   82] train_reward= -250.52, train_len= 191.1\n",
      "[Iter   83] train_reward= -244.75, train_len= 188.7\n",
      "[Iter   84] train_reward= -247.81, train_len= 189.3\n",
      "[Iter   85] train_reward= -238.27, train_len= 188.1\n",
      "[Iter   86] train_reward= -240.62, train_len= 187.9\n",
      "[Iter   87] train_reward= -230.35, train_len= 185.1\n",
      "[Iter   88] train_reward= -230.53, train_len= 184.7\n",
      "[Iter   89] train_reward= -223.38, train_len= 183.7\n",
      "[Iter   90] train_reward= -220.36, train_len= 185.3 | eval_return= -289.29, eval_len= 200.0\n",
      "[Iter   91] train_reward= -221.03, train_len= 186.4\n",
      "[Iter   92] train_reward= -222.09, train_len= 188.0\n",
      "[Iter   93] train_reward= -218.76, train_len= 187.1\n",
      "[Iter   94] train_reward= -213.75, train_len= 183.0\n",
      "[Iter   95] train_reward= -214.19, train_len= 182.1\n",
      "[Iter   96] train_reward= -218.41, train_len= 184.3\n",
      "[Iter   97] train_reward= -218.71, train_len= 183.9\n",
      "[Iter   98] train_reward= -218.67, train_len= 183.5\n",
      "[Iter   99] train_reward= -222.56, train_len= 182.9\n",
      "[Iter  100] train_reward= -221.74, train_len= 178.9 | eval_return= -263.92, eval_len= 200.0\n",
      "[Iter  101] train_reward= -221.15, train_len= 179.8\n",
      "[Iter  102] train_reward= -224.05, train_len= 181.3\n",
      "[Iter  103] train_reward= -223.02, train_len= 180.4\n",
      "[Iter  104] train_reward= -227.55, train_len= 184.3\n",
      "[Iter  105] train_reward= -224.49, train_len= 185.0\n",
      "[Iter  106] train_reward= -223.95, train_len= 187.1\n",
      "[Iter  107] train_reward= -219.69, train_len= 185.8\n",
      "[Iter  108] train_reward= -218.88, train_len= 185.9\n",
      "[Iter  109] train_reward= -220.93, train_len= 187.2\n",
      "[Iter  110] train_reward= -215.18, train_len= 184.8 | eval_return= -350.75, eval_len= 200.0\n",
      "[Iter  111] train_reward= -207.03, train_len= 181.1\n",
      "[Iter  112] train_reward= -207.92, train_len= 182.5\n",
      "[Iter  113] train_reward= -205.32, train_len= 179.7\n",
      "[Iter  114] train_reward= -203.93, train_len= 180.2\n",
      "[Iter  115] train_reward= -205.93, train_len= 180.0\n",
      "[Iter  116] train_reward= -207.32, train_len= 181.3\n",
      "[Iter  117] train_reward= -210.05, train_len= 182.3\n",
      "[Iter  118] train_reward= -205.61, train_len= 181.7\n",
      "[Iter  119] train_reward= -205.95, train_len= 182.1\n",
      "[Iter  120] train_reward= -212.13, train_len= 184.6 | eval_return= -332.38, eval_len= 200.0\n",
      "[Iter  121] train_reward= -219.12, train_len= 185.8\n",
      "[Iter  122] train_reward= -219.85, train_len= 186.7\n",
      "[Iter  123] train_reward= -226.71, train_len= 187.0\n",
      "[Iter  124] train_reward= -227.31, train_len= 185.0\n",
      "[Iter  125] train_reward= -217.51, train_len= 180.9\n",
      "[Iter  126] train_reward= -216.23, train_len= 180.2\n",
      "[Iter  127] train_reward= -218.70, train_len= 179.5\n",
      "[Iter  128] train_reward= -227.80, train_len= 183.1\n",
      "[Iter  129] train_reward= -235.33, train_len= 185.1\n",
      "[Iter  130] train_reward= -230.10, train_len= 183.2 | eval_return= -267.89, eval_len= 200.0\n",
      "[Iter  131] train_reward= -227.90, train_len= 184.4\n",
      "[Iter  132] train_reward= -224.59, train_len= 184.6\n",
      "[Iter  133] train_reward= -223.45, train_len= 187.7\n",
      "[Iter  134] train_reward= -224.14, train_len= 186.3\n",
      "[Iter  135] train_reward= -229.53, train_len= 188.4\n",
      "[Iter  136] train_reward= -223.49, train_len= 187.3\n",
      "[Iter  137] train_reward= -219.51, train_len= 186.0\n",
      "[Iter  138] train_reward= -213.80, train_len= 184.8\n",
      "[Iter  139] train_reward= -210.38, train_len= 183.2\n",
      "[Iter  140] train_reward= -209.33, train_len= 180.6 | eval_return= -245.24, eval_len= 200.0\n",
      "[Iter  141] train_reward= -207.78, train_len= 178.9\n",
      "[Iter  142] train_reward= -205.83, train_len= 175.3\n",
      "[Iter  143] train_reward= -206.05, train_len= 175.6\n",
      "[Iter  144] train_reward= -205.04, train_len= 176.6\n",
      "[Iter  145] train_reward= -207.44, train_len= 177.0\n",
      "[Iter  146] train_reward= -199.42, train_len= 174.2\n",
      "[Iter  147] train_reward= -195.05, train_len= 172.8\n",
      "[Iter  148] train_reward= -196.06, train_len= 174.0\n",
      "[Iter  149] train_reward= -196.40, train_len= 176.6\n",
      "[Iter  150] train_reward= -197.03, train_len= 178.1 | eval_return= -235.74, eval_len= 200.0\n",
      "  New best eval_return=-235.74, saved to TrainingResult(checkpoint=Checkpoint(filesystem=local, path=frog_transformer_checkpoints), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(1.368418407440186), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.506940160478864), 'policy_loss': np.float64(0.017355009807007653), 'vf_loss': np.float64(9.464532729557583), 'vf_explained_var': np.float64(-0.2700375420706613), 'kl': np.float64(0.018307585615132536), 'entropy': np.float64(6.013581527982439), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(11235.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 322000, 'num_env_steps_trained': 322000, 'num_agent_steps_sampled': 322000, 'num_agent_steps_trained': 322000}, 'env_runners': {'episode_reward_max': 9.704970143735409, 'episode_reward_min': -418.2430613040924, 'episode_reward_mean': np.float64(-197.02725804045795), 'episode_len_mean': np.float64(178.08), 'episode_media': {}, 'episodes_timesteps_total': 17808, 'policy_reward_min': {'default_policy': np.float64(-418.2430613040924)}, 'policy_reward_max': {'default_policy': np.float64(9.704970143735409)}, 'policy_reward_mean': {'default_policy': np.float64(-197.02725804045795)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-245.1205371618271, -386.9208674430847, -194.9586215019226, -265.91041028499603, -322.4797348976135, -259.98835384845734, -9.057036854326725, -16.252809882164, -99.14710879325867, -280.13209131360054, -348.2782157063484, -136.40855211019516, -260.72985580563545, -331.8948490023613, -35.401490435004234, -209.96315824985504, -44.1762430369854, -246.24372172355652, -248.0349133014679, -182.56278762221336, -276.9868820011616, -205.95998059213161, -105.76570338010788, -192.06795769929886, -40.44398509711027, -208.6488923728466, -384.7367697954178, -210.5032394528389, -173.99663013219833, -161.48994424939156, -174.807849496603, -143.10870397090912, -245.24805963039398, -128.19681897759438, -114.04343192279339, -196.80854865908623, -384.40966260433197, -314.66824346780777, -177.9155497252941, -253.01667541265488, 9.704970143735409, -207.39402344822884, -336.9115799665451, -58.97808815538883, -196.90952572226524, -223.6517854630947, -254.56896620988846, -307.56439965963364, -174.45637427270412, -198.77785955369473, -120.3961610943079, -205.0510701239109, -21.064563006162643, 5.93144515901804, -161.21791377663612, -179.2386989146471, -280.27360478043556, -282.84899508953094, 4.333415538072586, -170.19922068715096, -319.8952021598816, -165.18891167640686, -253.00987607240677, -207.98317497968674, -178.31338477134705, -249.7837028503418, -172.56674672663212, -202.075665473938, 7.8118606954813, -225.43096828460693, -179.930868268013, -102.14787262678146, -233.79848849773407, -327.2462167739868, -176.6512716561556, -229.07444974780083, -152.44144573807716, -184.99720135331154, -243.19620418548584, -181.86048075556755, -241.71779942512512, -193.55673590302467, -230.0914607644081, -141.3042547106743, -330.4186853170395, -198.20938989520073, -178.56121468544006, -159.633265376091, -154.73675647377968, -237.05238522589207, -92.04512355476618, -219.63773968815804, -186.35846328735352, -139.35161009430885, -418.2430613040924, -228.0736190378666, -236.2159287929535, -142.99684751033783, -322.809118270874, -97.91418612748384], 'episode_lengths': [200, 200, 200, 200, 200, 200, 27, 40, 105, 200, 200, 200, 200, 200, 50, 200, 90, 200, 200, 200, 200, 200, 163, 200, 62, 200, 200, 200, 200, 200, 200, 200, 200, 189, 200, 200, 200, 200, 200, 200, 2, 200, 200, 83, 200, 189, 200, 200, 200, 200, 139, 200, 56, 14, 200, 200, 200, 200, 17, 200, 200, 200, 200, 200, 186, 200, 200, 200, 9, 200, 180, 103, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 137, 200, 200, 200, 200, 200, 200, 200, 200, 167], 'policy_default_policy_reward': [-245.1205371618271, -386.9208674430847, -194.9586215019226, -265.91041028499603, -322.4797348976135, -259.98835384845734, -9.057036854326725, -16.252809882164, -99.14710879325867, -280.13209131360054, -348.2782157063484, -136.40855211019516, -260.72985580563545, -331.8948490023613, -35.401490435004234, -209.96315824985504, -44.1762430369854, -246.24372172355652, -248.0349133014679, -182.56278762221336, -276.9868820011616, -205.95998059213161, -105.76570338010788, -192.06795769929886, -40.44398509711027, -208.6488923728466, -384.7367697954178, -210.5032394528389, -173.99663013219833, -161.48994424939156, -174.807849496603, -143.10870397090912, -245.24805963039398, -128.19681897759438, -114.04343192279339, -196.80854865908623, -384.40966260433197, -314.66824346780777, -177.9155497252941, -253.01667541265488, 9.704970143735409, -207.39402344822884, -336.9115799665451, -58.97808815538883, -196.90952572226524, -223.6517854630947, -254.56896620988846, -307.56439965963364, -174.45637427270412, -198.77785955369473, -120.3961610943079, -205.0510701239109, -21.064563006162643, 5.93144515901804, -161.21791377663612, -179.2386989146471, -280.27360478043556, -282.84899508953094, 4.333415538072586, -170.19922068715096, -319.8952021598816, -165.18891167640686, -253.00987607240677, -207.98317497968674, -178.31338477134705, -249.7837028503418, -172.56674672663212, -202.075665473938, 7.8118606954813, -225.43096828460693, -179.930868268013, -102.14787262678146, -233.79848849773407, -327.2462167739868, -176.6512716561556, -229.07444974780083, -152.44144573807716, -184.99720135331154, -243.19620418548584, -181.86048075556755, -241.71779942512512, -193.55673590302467, -230.0914607644081, -141.3042547106743, -330.4186853170395, -198.20938989520073, -178.56121468544006, -159.633265376091, -154.73675647377968, -237.05238522589207, -92.04512355476618, -219.63773968815804, -186.35846328735352, -139.35161009430885, -418.2430613040924, -228.0736190378666, -236.2159287929535, -142.99684751033783, -322.809118270874, -97.91418612748384]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.0133339057257889), 'mean_inference_ms': np.float64(1.7859762042993754), 'mean_action_processing_ms': np.float64(0.4364832097217453), 'mean_env_wait_ms': np.float64(0.6733781804780893), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.004640102386474609), 'StateBufferConnector_ms': np.float64(0.004350423812866211), 'ViewRequirementAgentConnector_ms': np.float64(0.09251761436462402)}, 'num_episodes': 10, 'episode_return_max': 9.704970143735409, 'episode_return_min': -418.2430613040924, 'episode_return_mean': np.float64(-197.02725804045795), 'episodes_this_iter': 10}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 322000, 'num_agent_steps_trained': 322000, 'num_env_steps_sampled': 322000, 'num_env_steps_trained': 322000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 346.1782797870225, 'num_env_steps_trained_throughput_per_sec': 346.1782797870225, 'timesteps_total': 322000, 'num_env_steps_sampled_lifetime': 322000, 'num_agent_steps_sampled_lifetime': 322000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 322000, 'timers': {'training_iteration_time_ms': 6177.365, 'restore_workers_time_ms': 0.019, 'training_step_time_ms': 6177.307, 'sample_time_ms': 1995.024, 'load_time_ms': 0.673, 'load_throughput': 2971101.509, 'learn_time_ms': 4176.923, 'learn_throughput': 478.821, 'synch_weights_time_ms': 4.373}, 'counters': {'num_env_steps_sampled': 322000, 'num_env_steps_trained': 322000, 'num_agent_steps_sampled': 322000, 'num_agent_steps_trained': 322000}, 'done': False, 'training_iteration': 161, 'trial_id': 'default', 'date': '2025-12-02_02-39-33', 'timestamp': 1764661173, 'time_this_iter_s': 5.782453775405884, 'time_total_s': 987.7516632080078, 'pid': 2245261, 'hostname': 'platinumfish-Aspire', 'node_ip': '192.168.86.52', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'frog_hist_env', 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1, 'entropy_coeff': 0.01}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x784be534d360>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 987.7516632080078, 'iterations_since_restore': 161, 'perf': {'cpu_util_percent': np.float64(44.025000000000006), 'ram_util_percent': np.float64(65.9625)}})\n",
      "[Iter  151] train_reward= -187.61, train_len= 176.4\n",
      "[Iter  152] train_reward= -194.57, train_len= 179.4\n",
      "[Iter  153] train_reward= -200.55, train_len= 179.8\n",
      "[Iter  154] train_reward= -196.55, train_len= 180.2\n",
      "[Iter  155] train_reward= -201.86, train_len= 182.6\n",
      "[Iter  156] train_reward= -204.12, train_len= 183.3\n",
      "[Iter  157] train_reward= -207.36, train_len= 184.5\n",
      "[Iter  158] train_reward= -212.43, train_len= 181.6\n",
      "[Iter  159] train_reward= -214.56, train_len= 180.6\n",
      "[Iter  160] train_reward= -216.48, train_len= 183.7 | eval_return= -373.78, eval_len= 200.0\n",
      "[Iter  161] train_reward= -213.46, train_len= 183.3\n",
      "[Iter  162] train_reward= -211.03, train_len= 181.1\n",
      "[Iter  163] train_reward= -211.63, train_len= 179.3\n",
      "[Iter  164] train_reward= -210.39, train_len= 179.4\n",
      "[Iter  165] train_reward= -206.33, train_len= 177.6\n",
      "[Iter  166] train_reward= -209.51, train_len= 178.9\n",
      "[Iter  167] train_reward= -203.36, train_len= 178.5\n",
      "[Iter  168] train_reward= -198.21, train_len= 178.3\n",
      "[Iter  169] train_reward= -204.10, train_len= 179.6\n",
      "[Iter  170] train_reward= -207.15, train_len= 180.2 | eval_return= -226.01, eval_len= 200.0\n",
      "  New best eval_return=-226.01, saved to TrainingResult(checkpoint=Checkpoint(filesystem=local, path=frog_transformer_checkpoints), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(2.0526276111602795), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.097589240755354), 'policy_loss': np.float64(0.1927325127646327), 'vf_loss': np.float64(8.874203641074045), 'vf_explained_var': np.float64(0.012962922028132847), 'kl': np.float64(0.014933622096266065), 'entropy': np.float64(6.160689108712333), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(12635.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 362000, 'num_env_steps_trained': 362000, 'num_agent_steps_sampled': 362000, 'num_agent_steps_trained': 362000}, 'env_runners': {'episode_reward_max': 9.632389038801193, 'episode_reward_min': -382.290690779686, 'episode_reward_mean': np.float64(-207.14883082147688), 'episode_len_mean': np.float64(180.16), 'episode_media': {}, 'episodes_timesteps_total': 18016, 'policy_reward_min': {'default_policy': np.float64(-382.290690779686)}, 'policy_reward_max': {'default_policy': np.float64(9.632389038801193)}, 'policy_reward_mean': {'default_policy': np.float64(-207.14883082147688)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-39.15530440211296, -275.7238795161247, -171.02557748556137, -214.93632730841637, -242.4042066037655, -255.8907904624939, -344.32024323940277, 9.632389038801193, -288.64976209402084, -197.88665501773357, -184.35982728004456, -292.9073876738548, -83.29842820763588, -231.5274150967598, -267.580501973629, -188.69826114177704, -260.5930613875389, -361.63310945034027, -24.730036973953247, -33.2863504961133, -340.97837686538696, -197.16813576221466, -53.76338505744934, -236.64752197265625, -225.5318853855133, 4.777780137956142, -162.75883449614048, -226.48927882313728, -256.9900726675987, -186.03678566217422, -192.30734177678823, -193.94216220080853, -151.4606904014945, -201.78447552025318, -245.6144641637802, -194.2730928361416, -382.290690779686, -72.01996172964573, -193.5725405961275, -210.18489041924477, -117.59685772657394, -279.94695365428925, -155.52515345811844, -157.40050107240677, -305.95131862163544, -210.75625836849213, -249.93201887607574, -280.6427481174469, -228.1543905735016, -218.23228457570076, -282.6858393549919, -253.15419645607471, -149.79120998084545, -252.99355947971344, -329.24283039569855, -157.8359330892563, -305.641055226326, -202.64870235323906, -369.7341511249542, -290.93338710069656, -247.64621543884277, -45.1030957698822, -46.009741708636284, -239.3046289384365, -176.3435687571764, -81.62348201870918, -175.3137188255787, -245.0445694923401, -186.6624400317669, -228.888716340065, -208.22496336698532, -63.09015152230859, -152.14650037884712, -227.9246209859848, -217.62147465348244, -293.05895829200745, -29.89589999616146, -235.46887052059174, -228.03338792920113, -253.52915373444557, -235.1443445980549, -249.82930159568787, -175.2890768647194, -196.68276026844978, -166.69056391716003, -262.1880608201027, -240.94427835941315, -320.4341694712639, -158.01303926110268, -154.80460301041603, -104.40529131889343, -311.0146653652191, -226.45950192213058, -256.88281178474426, -309.02982234954834, -167.68744590878487, -229.34957873821259, -119.27993563562632, -244.11715903878212, -338.89161986112595], 'episode_lengths': [68, 200, 200, 200, 200, 200, 200, 2, 200, 200, 200, 200, 119, 200, 200, 200, 200, 200, 49, 74, 200, 200, 95, 200, 200, 15, 200, 200, 200, 200, 176, 200, 148, 193, 200, 200, 200, 100, 200, 200, 126, 200, 200, 200, 200, 187, 200, 200, 200, 200, 200, 200, 200, 200, 200, 156, 200, 200, 200, 200, 200, 82, 111, 200, 200, 70, 200, 200, 200, 200, 200, 123, 200, 200, 200, 200, 55, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 102, 200, 200, 200, 200, 200, 200, 165, 200, 200], 'policy_default_policy_reward': [-39.15530440211296, -275.7238795161247, -171.02557748556137, -214.93632730841637, -242.4042066037655, -255.8907904624939, -344.32024323940277, 9.632389038801193, -288.64976209402084, -197.88665501773357, -184.35982728004456, -292.9073876738548, -83.29842820763588, -231.5274150967598, -267.580501973629, -188.69826114177704, -260.5930613875389, -361.63310945034027, -24.730036973953247, -33.2863504961133, -340.97837686538696, -197.16813576221466, -53.76338505744934, -236.64752197265625, -225.5318853855133, 4.777780137956142, -162.75883449614048, -226.48927882313728, -256.9900726675987, -186.03678566217422, -192.30734177678823, -193.94216220080853, -151.4606904014945, -201.78447552025318, -245.6144641637802, -194.2730928361416, -382.290690779686, -72.01996172964573, -193.5725405961275, -210.18489041924477, -117.59685772657394, -279.94695365428925, -155.52515345811844, -157.40050107240677, -305.95131862163544, -210.75625836849213, -249.93201887607574, -280.6427481174469, -228.1543905735016, -218.23228457570076, -282.6858393549919, -253.15419645607471, -149.79120998084545, -252.99355947971344, -329.24283039569855, -157.8359330892563, -305.641055226326, -202.64870235323906, -369.7341511249542, -290.93338710069656, -247.64621543884277, -45.1030957698822, -46.009741708636284, -239.3046289384365, -176.3435687571764, -81.62348201870918, -175.3137188255787, -245.0445694923401, -186.6624400317669, -228.888716340065, -208.22496336698532, -63.09015152230859, -152.14650037884712, -227.9246209859848, -217.62147465348244, -293.05895829200745, -29.89589999616146, -235.46887052059174, -228.03338792920113, -253.52915373444557, -235.1443445980549, -249.82930159568787, -175.2890768647194, -196.68276026844978, -166.69056391716003, -262.1880608201027, -240.94427835941315, -320.4341694712639, -158.01303926110268, -154.80460301041603, -104.40529131889343, -311.0146653652191, -226.45950192213058, -256.88281178474426, -309.02982234954834, -167.68744590878487, -229.34957873821259, -119.27993563562632, -244.11715903878212, -338.89161986112595]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.0198796243656492), 'mean_inference_ms': np.float64(1.7969871030854543), 'mean_action_processing_ms': np.float64(0.43819740181340255), 'mean_env_wait_ms': np.float64(0.6760478795164263), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.005621910095214844), 'StateBufferConnector_ms': np.float64(0.00521397590637207), 'ViewRequirementAgentConnector_ms': np.float64(0.10386037826538086)}, 'num_episodes': 11, 'episode_return_max': 9.632389038801193, 'episode_return_min': -382.290690779686, 'episode_return_mean': np.float64(-207.14883082147688), 'episodes_this_iter': 11}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 362000, 'num_agent_steps_trained': 362000, 'num_env_steps_sampled': 362000, 'num_env_steps_trained': 362000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 320.52760185317857, 'num_env_steps_trained_throughput_per_sec': 320.52760185317857, 'timesteps_total': 362000, 'num_env_steps_sampled_lifetime': 362000, 'num_agent_steps_sampled_lifetime': 362000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 362000, 'timers': {'training_iteration_time_ms': 6955.584, 'restore_workers_time_ms': 0.022, 'training_step_time_ms': 6955.514, 'sample_time_ms': 2276.126, 'load_time_ms': 0.807, 'load_throughput': 2477804.756, 'learn_time_ms': 4673.298, 'learn_throughput': 427.963, 'synch_weights_time_ms': 4.888}, 'counters': {'num_env_steps_sampled': 362000, 'num_env_steps_trained': 362000, 'num_agent_steps_sampled': 362000, 'num_agent_steps_trained': 362000}, 'done': False, 'training_iteration': 181, 'trial_id': 'default', 'date': '2025-12-02_02-41-49', 'timestamp': 1764661309, 'time_this_iter_s': 6.247241020202637, 'time_total_s': 1119.6109223365784, 'pid': 2245261, 'hostname': 'platinumfish-Aspire', 'node_ip': '192.168.86.52', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'frog_hist_env', 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1, 'entropy_coeff': 0.01}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x784be534d360>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 20, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 1119.6109223365784, 'iterations_since_restore': 181, 'perf': {'cpu_util_percent': np.float64(49.8625), 'ram_util_percent': np.float64(66.0375)}})\n",
      "[Iter  171] train_reward= -209.33, train_len= 181.6\n",
      "[Iter  172] train_reward= -207.45, train_len= 181.7\n",
      "[Iter  173] train_reward= -212.03, train_len= 183.9\n",
      "[Iter  174] train_reward= -216.98, train_len= 186.3\n",
      "[Iter  175] train_reward= -211.90, train_len= 181.2\n",
      "[Iter  176] train_reward= -214.24, train_len= 183.1\n",
      "[Iter  177] train_reward= -218.89, train_len= 183.7\n",
      "[Iter  178] train_reward= -212.02, train_len= 180.6\n",
      "[Iter  179] train_reward= -209.02, train_len= 179.0\n",
      "[Iter  180] train_reward= -205.20, train_len= 178.6 | eval_return= -267.07, eval_len= 200.0\n",
      "[Iter  181] train_reward= -210.35, train_len= 182.0\n",
      "[Iter  182] train_reward= -207.79, train_len= 180.3\n",
      "[Iter  183] train_reward= -207.86, train_len= 179.7\n",
      "[Iter  184] train_reward= -203.48, train_len= 180.6\n",
      "[Iter  185] train_reward= -207.49, train_len= 182.8\n",
      "[Iter  186] train_reward= -207.61, train_len= 184.0\n",
      "[Iter  187] train_reward= -209.71, train_len= 185.1\n",
      "[Iter  188] train_reward= -210.22, train_len= 185.6\n",
      "[Iter  189] train_reward= -211.40, train_len= 186.9\n",
      "[Iter  190] train_reward= -209.55, train_len= 183.3 | eval_return= -333.75, eval_len= 200.0\n",
      "[Iter  191] train_reward= -210.24, train_len= 182.7\n",
      "[Iter  192] train_reward= -211.12, train_len= 183.3\n",
      "[Iter  193] train_reward= -210.01, train_len= 183.1\n",
      "[Iter  194] train_reward= -202.70, train_len= 178.3\n",
      "[Iter  195] train_reward= -200.07, train_len= 174.8\n",
      "[Iter  196] train_reward= -198.67, train_len= 175.2\n",
      "[Iter  197] train_reward= -205.31, train_len= 177.5\n",
      "[Iter  198] train_reward= -201.50, train_len= 173.7\n",
      "[Iter  199] train_reward= -206.35, train_len= 177.3\n",
      "[Iter  200] train_reward= -208.72, train_len= 180.4 | eval_return= -300.66, eval_len= 200.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_iterations = 200        # how long you want to train\n",
    "eval_interval  = 10         # how often to run eval\n",
    "eval_episodes  = 5\n",
    "\n",
    "train_history = []          # for plotting later\n",
    "best_eval_return = -float(\"inf\")\n",
    "checkpoint_dir = \"frog_transformer_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, num_iterations + 1):\n",
    "    # ---- 1. One PPO training iteration ----\n",
    "    result = algo.train()\n",
    "    train_reward, train_len = get_train_metrics(result)\n",
    "\n",
    "    # ---- 2. Periodic evaluation ----\n",
    "    if i % eval_interval == 0:\n",
    "        eval_stats = evaluate_policy(\n",
    "            algo,\n",
    "            eval_env,\n",
    "            num_episodes=eval_episodes,\n",
    "            render=False,\n",
    "        )\n",
    "        eval_return = eval_stats[\"mean_return\"]\n",
    "        eval_len    = eval_stats[\"mean_length\"]\n",
    "\n",
    "        print(\n",
    "            f\"[Iter {i:4d}] \"\n",
    "            f\"train_reward={train_reward:8.2f}, \"\n",
    "            f\"train_len={train_len:6.1f} | \"\n",
    "            f\"eval_return={eval_return:8.2f}, \"\n",
    "            f\"eval_len={eval_len:6.1f}\"\n",
    "        )\n",
    "\n",
    "        # Save history for plotting\n",
    "        train_history.append(\n",
    "            {\n",
    "                \"iter\": i,\n",
    "                \"train_reward\": train_reward,\n",
    "                \"train_len\": train_len,\n",
    "                \"eval_return\": eval_return,\n",
    "                \"eval_len\": eval_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # save best checkpoint\n",
    "        if eval_return > best_eval_return:\n",
    "            best_eval_return = eval_return\n",
    "            checkpoint_path = algo.save(checkpoint_dir)\n",
    "            print(f\"  New best eval_return={best_eval_return:.2f}, saved to {checkpoint_path}\")\n",
    "    else:\n",
    "        # Lighter log on non-eval iterations\n",
    "        print(\n",
    "            f\"[Iter {i:4d}] \"\n",
    "            f\"train_reward={train_reward:8.2f}, \"\n",
    "            f\"train_len={train_len:6.1f}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_hist = pd.DataFrame(train_history)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_hist[\"iter\"], df_hist[\"train_reward\"], label=\"train reward\")\n",
    "plt.plot(df_hist[\"iter\"], df_hist[\"eval_return\"], label=\"eval return\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Episode return\")\n",
    "plt.legend()\n",
    "plt.title(\"Frog Transformer PPO training\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3D GIF to: transformer_original_3d.gif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-12-02 15:59:31,681 E 2248330 2248330] (raylet) node_manager.cc:3277: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 4f5df2e60efa7da9b72e6572929ecde932b3c32d83c84c30aeafac70, IP: 192.168.86.52) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.86.52`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. UNWRAP ENV (in case it's inside ObsHistoryWrapper)\n",
    "# ------------------------------------------------------------\n",
    "def unwrap_env(env):\n",
    "    \"\"\"Return underlying FrogFly3DEnv even if wrapped.\"\"\"\n",
    "    while hasattr(env, \"env\"):\n",
    "        env = env.env\n",
    "    return env\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. ROLLOUT AND RECORD POSITIONS\n",
    "# ------------------------------------------------------------\n",
    "def rollout_trajectory(env, algo, max_steps=200):\n",
    "    raw_env = unwrap_env(env)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    frog_traj = [raw_env.frog_pos.copy()]\n",
    "    fly_traj  = [raw_env.fly_pos.copy()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action, _, _ = algo.compute_single_action(obs, explore=False)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        frog_traj.append(raw_env.frog_pos.copy())\n",
    "        fly_traj.append(raw_env.fly_pos.copy())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return np.array(frog_traj), np.array(fly_traj)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. CREATE 3D ANIMATION\n",
    "# ------------------------------------------------------------\n",
    "def make_3d_gif(frog_traj, fly_traj, gif_path=\"transformer_original_3d.gif\"):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # initial points\n",
    "    frog_dot, = ax.plot([], [], [], 'go', markersize=8)\n",
    "    fly_dot,  = ax.plot([], [], [], 'ro', markersize=8)\n",
    "\n",
    "    # history lines\n",
    "    frog_path, = ax.plot([], [], [], 'g-', linewidth=2, alpha=0.7)\n",
    "    fly_path,  = ax.plot([], [], [], 'r-', linewidth=2, alpha=0.7)\n",
    "\n",
    "    # axis limits\n",
    "    ax.set_xlim([-1.0, 1.0])\n",
    "    ax.set_ylim([-1.0, 1.0])\n",
    "    ax.set_zlim([-1.0, 1.0])\n",
    "    ax.set_title(\"Frog-Fly 3D Trajectory (Original Transformer PPO)\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "    def init():\n",
    "        frog_dot.set_data([], [])\n",
    "        frog_dot.set_3d_properties([])\n",
    "        fly_dot.set_data([], [])\n",
    "        fly_dot.set_3d_properties([])\n",
    "        frog_path.set_data([], [])\n",
    "        frog_path.set_3d_properties([])\n",
    "        fly_path.set_data([], [])\n",
    "        fly_path.set_3d_properties([])\n",
    "        return frog_dot, fly_dot, frog_path, fly_path\n",
    "\n",
    "    def update(i):\n",
    "        # --- frog point ---\n",
    "        frog_dot.set_data([frog_traj[i, 0]], [frog_traj[i, 1]])\n",
    "        frog_dot.set_3d_properties([frog_traj[i, 2]])\n",
    "\n",
    "        # --- fly point ---\n",
    "        fly_dot.set_data([fly_traj[i, 0]], [fly_traj[i, 1]])\n",
    "        fly_dot.set_3d_properties([fly_traj[i, 2]])\n",
    "\n",
    "        # --- frog path ---\n",
    "        frog_path.set_data(frog_traj[:i+1, 0], frog_traj[:i+1, 1])\n",
    "        frog_path.set_3d_properties(frog_traj[:i+1, 2])\n",
    "\n",
    "        # --- fly path ---\n",
    "        fly_path.set_data(fly_traj[:i+1, 0], fly_traj[:i+1, 1])\n",
    "        fly_path.set_3d_properties(fly_traj[:i+1, 2])\n",
    "\n",
    "        return frog_dot, fly_dot, frog_path, fly_path\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, init_func=init,\n",
    "        frames=len(frog_traj),\n",
    "        interval=60,\n",
    "        blit=False  # IMPORTANT for 3D\n",
    "    )\n",
    "\n",
    "    ani.save(gif_path, writer=\"pillow\", fps=20)\n",
    "\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved 3D GIF to: {gif_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. RUN IT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# STEP A — create a fresh evaluation env (unwrapped)\n",
    "eval_env = FrogFly3DEnv(env_config)\n",
    "\n",
    "# STEP B — rollout using the trained algorithm \"algo\"\n",
    "frog_traj, fly_traj = rollout_trajectory(eval_env, algo)\n",
    "\n",
    "# STEP C — render the GIF\n",
    "make_3d_gif(frog_traj, fly_traj, gif_path=\"transformer_original_3d.gif\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
