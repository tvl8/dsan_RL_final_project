{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 PPO + Transformer-Based Policy Network\n",
    "\n",
    "To incorporate temporal context into PPO, we replace the MLP policy with a Transformer policy.  \n",
    "Instead of conditioning only on the current state, the agent receives a short history of experience and attends to relevant past transitions.\n",
    "\n",
    "---\n",
    "\n",
    "### Sequential Input Representation\n",
    "\n",
    "At timestep \\(t\\), we construct a window of recent experience:\n",
    "\n",
    "$$\n",
    "\\mathcal{E}_t = \\big[(s_{t-T+1}, a_{t-T+1}), \\dots, (s_{t-1},a_{t-1}), (s_t,a_t)\\big]\n",
    "$$\n",
    "\n",
    "Each element is embedded using a small MLP:\n",
    "\n",
    "$$\n",
    "x_i = \\mathrm{MLP}_{\\text{embed}}([s_i, a_i])\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Temporal Positional Encoding\n",
    "\n",
    "Because self-attention is order-invariant, we inject temporal indices using sinusoidal encodings:\n",
    "\n",
    "$$\n",
    "PE(i) = [\\sin(\\omega_k i), \\cos(\\omega_k i)]_{k=1}^{F}\n",
    "$$\n",
    "\n",
    "The input token for the Transformer becomes:\n",
    "\n",
    "$$\n",
    "z_i = x_i \\;\\Vert\\; PE(i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Transformer Encoder Block\n",
    "\n",
    "A Transformer layer processes the sequence by attending over past and recent experience:\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\mathrm{LN}\\Big(H^{(l)} +\n",
    "\\mathrm{MHA}(H^{(l)}, H^{(l)}, H^{(l)})\\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\mathrm{LN}\\Big(H^{(l)} + \\mathrm{FFN}(H^{(l)})\\Big)\n",
    "$$\n",
    "\n",
    "The final hidden state \\(H_T\\) serves as a context summary for decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### Policy & Value Outputs\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(a_t|\\mathcal{E}_t) =\n",
    "\\mathrm{Softmax}(\\mathrm{MLP}_{\\text{policy}}(H_T))\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_\\phi(\\mathcal{E}_t) =\n",
    "\\mathrm{MLP}_{\\text{value}}(H_T)\n",
    "$$\n",
    "\n",
    "This allows PPO to act based on *recent experience*, not just a single observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (needed for 3D)\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_3d(pos, num_freqs=4, max_freq=10.0):\n",
    "    \"\"\"\n",
    "    pos: np.array shape (3,) with (x, y, z) in [-space_size, space_size].\n",
    "    Returns: encoding vector of shape (3 * 2 * num_freqs,)\n",
    "    \"\"\"\n",
    "    pos = np.asarray(pos, dtype=np.float32)\n",
    "    assert pos.shape == (3,)\n",
    "\n",
    "    freqs = np.linspace(1.0, max_freq, num_freqs)\n",
    "    enc = []\n",
    "    for coord in pos:            # x, y, z\n",
    "        for f in freqs:\n",
    "            enc.append(np.sin(f * coord))\n",
    "            enc.append(np.cos(f * coord))\n",
    "    return np.array(enc, dtype=np.float32)\n",
    "\n",
    "\n",
    "class FrogFly3DEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    3D continuous environment:\n",
    "    - Frog (agent) moves to catch a moving fly.\n",
    "    - Observations: positions + relative vector + positional encodings.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            config = {}\n",
    "\n",
    "        self.space_size = float(config.get(\"space_size\", 1.0))\n",
    "        self.step_size = float(config.get(\"step_size\", 0.1))\n",
    "        self.fly_speed = float(config.get(\"fly_speed\", 0.05))\n",
    "        self.catch_radius = float(config.get(\"catch_radius\", 0.15))\n",
    "        self.max_steps = int(config.get(\"max_steps\", 200))\n",
    "        self.num_freqs = int(config.get(\"num_freqs\", 4))\n",
    "        self.use_positional_encodings = bool(\n",
    "            config.get(\"use_positional_encodings\", True)\n",
    "        )\n",
    "\n",
    "        # Action = 3D movement in [-1, 1]^3\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(3,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation components\n",
    "        # frog_pos (3) + fly_pos (3) + rel (3) = 9\n",
    "        base_dim = 9\n",
    "        if self.use_positional_encodings:\n",
    "            pe_dim = 3 * 2 * self.num_freqs  # per position\n",
    "            obs_dim = base_dim + 2 * pe_dim\n",
    "        else:\n",
    "            obs_dim = base_dim\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-5.0, high=5.0, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.frog_pos = None\n",
    "        self.fly_pos = None\n",
    "        self.steps = 0\n",
    "        self._value_out = None\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "\n",
    "    def _sample_position(self):\n",
    "        return np.random.uniform(\n",
    "            low=-self.space_size,\n",
    "            high=self.space_size,\n",
    "            size=(3,),\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def _clip_position(self, pos):\n",
    "        return np.clip(pos, -self.space_size, self.space_size).astype(np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        rel = self.fly_pos - self.frog_pos\n",
    "        parts = [self.frog_pos, self.fly_pos, rel]\n",
    "\n",
    "        if self.use_positional_encodings:\n",
    "            frog_pe = positional_encoding_3d(self.frog_pos, self.num_freqs)\n",
    "            fly_pe = positional_encoding_3d(self.fly_pos, self.num_freqs)\n",
    "            parts.extend([frog_pe, fly_pe])\n",
    "\n",
    "        return np.concatenate(parts, axis=0).astype(np.float32)\n",
    "\n",
    "    def _get_distance(self):\n",
    "        return float(np.linalg.norm(self.fly_pos - self.frog_pos))\n",
    "\n",
    "    # ---------- Gymnasium API ----------\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.frog_pos = self._sample_position()\n",
    "        self.fly_pos = self._sample_position()\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        action = np.asarray(action, dtype=np.float32)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        # -------------------------\n",
    "        # Distance BEFORE movement\n",
    "        # -------------------------\n",
    "        prev_dist = self._get_distance()\n",
    "\n",
    "        # -------------------------\n",
    "        # Frog movement\n",
    "        # -------------------------\n",
    "        self.frog_pos = self.frog_pos + self.step_size * action\n",
    "        self.frog_pos = self._clip_position(self.frog_pos)\n",
    "\n",
    "        # -------------------------\n",
    "        # Fly movement\n",
    "        # -------------------------\n",
    "        fly_dir = np.random.normal(size=(3,)).astype(np.float32)\n",
    "        fly_dir /= np.linalg.norm(fly_dir) + 1e-8\n",
    "        self.fly_pos = self.fly_pos + self.fly_speed * fly_dir\n",
    "        self.fly_pos = self._clip_position(self.fly_pos)\n",
    "\n",
    "        # -------------------------\n",
    "        # AFTER movement\n",
    "        # -------------------------\n",
    "        dist = self._get_distance()\n",
    "        caught = dist < self.catch_radius\n",
    "\n",
    "        # -------------------------\n",
    "        # BASE REWARD (distance improvement)\n",
    "        # -------------------------\n",
    "        dist_delta = prev_dist - dist\n",
    "        reward = dist_delta\n",
    "\n",
    "        # small time penalty to discourage wandering\n",
    "        reward -= 0.01\n",
    "\n",
    "        # -------------------------\n",
    "        # ALIGNMENT BONUS (STRONG)\n",
    "        # -------------------------\n",
    "        direction = self.fly_pos - self.frog_pos\n",
    "        dir_norm = np.linalg.norm(direction)\n",
    "        act_norm = np.linalg.norm(action)\n",
    "\n",
    "        if dir_norm > 1e-6 and act_norm > 1e-6:\n",
    "            alignment = np.dot(action / act_norm, direction / dir_norm)\n",
    "            reward += 0.4 * alignment   # <<< STRONG BUT NOT EXPLOSIVE\n",
    "\n",
    "        # -------------------------\n",
    "        # Catch bonus\n",
    "        # -------------------------\n",
    "        if caught:\n",
    "            reward += 5.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {\n",
    "            \"distance\": dist,\n",
    "            \"distance_delta\": dist_delta,\n",
    "            \"caught\": caught,\n",
    "        }\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"Step {self.steps} | \"\n",
    "            f\"Frog: {self.frog_pos} | Fly: {self.fly_pos} | \"\n",
    "            f\"Dist: {self._get_distance():.3f}\"\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered frog_hist_env OK\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "# K-step observation history wrapper (if you don't already have this)\n",
    "class ObsHistoryWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, K=24):\n",
    "        super().__init__(env)\n",
    "        self.K = K\n",
    "        obs_shape = env.observation_space.shape  # (D,)\n",
    "        self.history = np.zeros((K,) + obs_shape, dtype=np.float32)\n",
    "\n",
    "        low  = np.repeat(env.observation_space.low[None, :],  K, axis=0)\n",
    "        high = np.repeat(env.observation_space.high[None, :], K, axis=0)\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.history[:] = obs\n",
    "        return self.history.copy(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.history = np.roll(self.history, shift=-1, axis=0)\n",
    "        self.history[-1] = obs\n",
    "        return self.history.copy(), reward, terminated, truncated, info\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def make_frog_hist_env(env_config):\n",
    "    return ObsHistoryWrapper(FrogFly3DEnv(env_config), K=24)\n",
    "\n",
    "register_env(\"frog_hist_env\", make_frog_hist_env)\n",
    "\n",
    "print(\"Registered frog_hist_env OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:20:18,620\tINFO worker.py:2023 -- Started a local Ray instance.\n",
      "/home/platinumfish/miniconda3/envs/rl-project/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "class FrogTransformerModel(TorchModelV2, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        # Expect obs shape = (K, obs_dim_single)\n",
    "        assert len(obs_space.shape) == 2, f\"Expected (K, D), got {obs_space.shape}\"\n",
    "        self.seq_len    = obs_space.shape[0]  # K (time)\n",
    "        self.feature_dim = obs_space.shape[1] # D (per-timestep features)\n",
    "\n",
    "        cmc = model_config.get(\"custom_model_config\", {}) or {}\n",
    "\n",
    "        def get_hp(key, default):\n",
    "            return kwargs.get(key, cmc.get(key, default))\n",
    "\n",
    "        d_model    = get_hp(\"d_model\", 64)\n",
    "        nhead      = get_hp(\"nhead\", 4)\n",
    "        num_layers = get_hp(\"num_layers\", 1)\n",
    "        dim_ff     = get_hp(\"dim_feedforward\", 128)\n",
    "        dropout    = get_hp(\"dropout\", 0.1)\n",
    "\n",
    "        # Embedding per timestep (map R^{D} -> R^{d_model})\n",
    "        self.embed = nn.Linear(self.feature_dim, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_outputs),\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "        self._features = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Get obs as float tensor, whatever its shape is\n",
    "        x = input_dict[\"obs\"].float()       # shape: (B, ...) with possibly >2 dims\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Flatten all non-batch dims into a single feature dimension\n",
    "        x = x.view(B, -1)                   # shape: (B, F_flat)\n",
    "        F = x.shape[1]\n",
    "\n",
    "        # --- Infer how to interpret the obs as (B, K, D) sequence ---\n",
    "        # Case 1: env gives a single feature vector of length feature_dim → treat as K=1\n",
    "        if F == self.feature_dim:\n",
    "            K = 1\n",
    "        # Case 2: env gives a flattened sequence of length seq_len * feature_dim\n",
    "        elif F == self.seq_len * self.feature_dim:\n",
    "            K = self.seq_len\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Cannot reshape obs of size {F} into \"\n",
    "                f\"(B, K, feature_dim={self.feature_dim}) with seq_len={self.seq_len}.\"\n",
    "            )\n",
    "\n",
    "        # Reshape to (B, K, D)\n",
    "        x = x.view(B, K, self.feature_dim)\n",
    "\n",
    "        # --- Standard Transformer forward ---\n",
    "        # (B, K, D) → (B, K, d_model)\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # Optional: positional encodings, if you defined self.pos_embed\n",
    "        if hasattr(self, \"pos_embed\"):\n",
    "            # assume pos_embed has shape (1, max_K, d_model)\n",
    "            x = x + self.pos_embed[:, :K, :]\n",
    "\n",
    "        # TransformerEncoder expects (K, B, d_model)\n",
    "        x = x.transpose(0, 1)          # (K, B, d_model)\n",
    "        x = self.transformer(x)        # (K, B, d_model)\n",
    "        x = x.transpose(0, 1)          # (B, K, d_model)\n",
    "\n",
    "        # Pooling: take last token\n",
    "        token = x[:, -1, :]            # (B, d_model)\n",
    "\n",
    "        # Policy and value heads\n",
    "        logits = self.policy_head(token)     # (B, num_actions)\n",
    "        value  = self.value_head(token)      # (B, 1)\n",
    "\n",
    "        # RLlib expects value via this attribute\n",
    "        self._value_out = value.squeeze(-1)  # (B,)\n",
    "\n",
    "        # Return logits + empty RNN state list\n",
    "        return logits, []\n",
    "\n",
    "    def value_function(self):\n",
    "        # RLlib calls this after forward() to get V(s).\n",
    "        if self._value_out is None:\n",
    "            raise ValueError(\"value_function() called before forward().\")\n",
    "        return self._value_out\n",
    "\n",
    "ModelCatalog.register_custom_model(\"frog_transformer_policy\", FrogTransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frog_env(env_config):\n",
    "    base_env = FrogFly3DEnv(env_config)\n",
    "    return ObsHistoryWrapper(base_env, K=24)  # e.g., last 8 obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'resources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m      3\u001b[0m env_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_positional_encodings\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# PPO CONFIG — EXACT MATCH TO TRAINING API STACK\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# (minimal settings needed for restore + rollout)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     17\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     18\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrog_hist_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_stack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_rl_module_and_learner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# <<< CRITICAL\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_rollout_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# local-only rollout (no env runners)\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_fragment_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# matches max_steps but does not matter\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m(\n\u001b[1;32m     33\u001b[0m         num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(\n\u001b[1;32m     36\u001b[0m         model\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrog_transformer_policy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_model_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     39\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnhead\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     41\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_feedforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     43\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m     45\u001b[0m             },\n\u001b[1;32m     46\u001b[0m         }\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     50\u001b[0m algo \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgo built OK with grad clipping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'resources'"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_config = {\n",
    "    \"space_size\": 1.0,\n",
    "    \"step_size\": 0.1,\n",
    "    \"fly_speed\": 0.05,\n",
    "    \"catch_radius\": 0.15,\n",
    "    \"max_steps\": 200,\n",
    "    \"num_freqs\": 4,\n",
    "    \"use_positional_encodings\": True,\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# PPO CONFIG — EXACT MATCH TO TRAINING API STACK\n",
    "# (minimal settings needed for restore + rollout)\n",
    "# -------------------------------------------------------\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"frog_hist_env\",\n",
    "        env_config=env_config,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False,   # <<< CRITICAL\n",
    "    )\n",
    "    .rollouts(\n",
    "        num_rollout_workers=0,          # local-only rollout (no env runners)\n",
    "        rollout_fragment_length=200,    # matches max_steps but does not matter\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=0\n",
    "    )\n",
    "    .training(\n",
    "        model={\n",
    "            \"custom_model\": \"frog_transformer_policy\",\n",
    "            \"custom_model_config\": {\n",
    "                \"d_model\": 128,\n",
    "                \"nhead\": 4,\n",
    "                \"num_layers\": 2,\n",
    "                \"dim_feedforward\": 256,\n",
    "                \"dropout\": 0.05,\n",
    "                \"seq_len\": 24,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(\"Algo built OK with grad clipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 17:20:30,468\tWARNING train_ops.py:114 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n",
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-02 17:20:47,636 E 51742 51742] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-12-02 17:20:48,557 E 51852 51852] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(RolloutWorker pid=51898)\u001b[0m [2025-12-02 17:20:49,397 E 51898 51996] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-02 17:20:49,692 E 51038 51897] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['custom_metrics', 'episode_media', 'info', 'env_runners', 'num_healthy_workers', 'actor_manager_num_outstanding_async_reqs', 'num_remote_worker_restarts', 'num_agent_steps_sampled', 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'num_env_steps_sampled_throughput_per_sec', 'num_env_steps_trained_throughput_per_sec', 'timesteps_total', 'num_env_steps_sampled_lifetime', 'num_agent_steps_sampled_lifetime', 'num_steps_trained_this_iter', 'agent_timesteps_total', 'timers', 'counters', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore', 'perf'])\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(0.09999999999999998), 'cur_lr': np.float64(3.0000000000000004e-05), 'total_loss': np.float64(6.55755184173584), 'policy_loss': np.float64(0.09188978439817826), 'vf_loss': np.float64(6.463491840362549), 'vf_explained_var': np.float64(-0.0302687668800354), 'kl': np.float64(0.24799645880858104), 'entropy': np.float64(4.525883172353109), 'entropy_coeff': np.float64(0.005)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(38.0), 'diff_num_grad_updates_vs_sampler_policy': np.float64(37.0)}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'env_runners': {'episode_reward_max': np.float32(5.0930243), 'episode_reward_min': np.float32(-28.58943), 'episode_reward_mean': np.float32(-11.466465), 'episode_len_mean': np.float64(190.9), 'episode_media': {}, 'episodes_timesteps_total': 3818, 'policy_reward_min': {'default_policy': np.float32(-28.58943)}, 'policy_reward_max': {'default_policy': np.float32(5.0930243)}, 'policy_reward_mean': {'default_policy': np.float32(-11.466465)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [np.float32(-10.708922), np.float32(-2.3138661), np.float32(-6.5944505), np.float32(-18.126028), np.float32(5.0930243), np.float32(-11.063968), np.float32(-8.619887), np.float32(-14.114965), np.float32(-28.58943), np.float32(-9.81493), np.float32(-8.167291), np.float32(-11.473765), np.float32(-11.02109), np.float32(-17.172466), np.float32(-11.729132), np.float32(-11.143374), np.float32(-5.206301), np.float32(-24.005701), np.float32(-6.1039224), np.float32(-18.452816)], 'episode_lengths': [200, 200, 200, 200, 18, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_default_policy_reward': [np.float32(-10.708922), np.float32(-2.3138661), np.float32(-6.5944505), np.float32(-18.126028), np.float32(5.0930243), np.float32(-11.063968), np.float32(-8.619887), np.float32(-14.114965), np.float32(-28.58943), np.float32(-9.81493), np.float32(-8.167291), np.float32(-11.473765), np.float32(-11.02109), np.float32(-17.172466), np.float32(-11.729132), np.float32(-11.143374), np.float32(-5.206301), np.float32(-24.005701), np.float32(-6.1039224), np.float32(-18.452816)]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.9652796563330467), 'mean_inference_ms': np.float64(3.344392443036699), 'mean_action_processing_ms': np.float64(0.42246700405002713), 'mean_env_wait_ms': np.float64(0.7005690575598716), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.006167888641357422), 'StateBufferConnector_ms': np.float64(0.015789270401000977), 'ViewRequirementAgentConnector_ms': np.float64(0.0948190689086914)}, 'num_episodes': 20, 'episode_return_max': np.float32(5.0930243), 'episode_return_min': np.float32(-28.58943), 'episode_return_mean': np.float32(-11.466465), 'episodes_this_iter': 20}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 106.81251400754202, 'num_env_steps_trained_throughput_per_sec': 106.81251400754202, 'timesteps_total': 4000, 'num_env_steps_sampled_lifetime': 4000, 'num_agent_steps_sampled_lifetime': 4000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 37448.811, 'restore_workers_time_ms': 0.023, 'training_step_time_ms': 37448.743, 'sample_time_ms': 5536.217, 'load_time_ms': 0.28, 'load_throughput': 14290643.952, 'learn_time_ms': 31906.661, 'learn_throughput': 125.366, 'synch_weights_time_ms': 4.553}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-12-02_17-21-02', 'timestamp': 1764714062, 'time_this_iter_s': 37.45435929298401, 'time_total_s': 37.45435929298401, 'pid': 51038, 'hostname': 'platinumfish-Aspire', 'node_ip': '192.168.86.52', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'frog_hist_env', 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 3e-05, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 5, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 128, 'nhead': 4, 'num_layers': 2, 'dim_feedforward': 256, 'dropout': 0.05, 'seq_len': 24}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x74c44c805510>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.1, 'kl_target': 0.02, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.005, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 37.45435929298401, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(48.112962962962975), 'ram_util_percent': np.float64(49.53148148148148)}}\n"
     ]
    }
   ],
   "source": [
    "res = algo.train()\n",
    "print(res.keys())\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metrics(result):\n",
    "    \"\"\"Extract mean reward and len from an RLlib result dict (env_runners layout).\"\"\"\n",
    "    er = result.get(\"env_runners\", {})\n",
    "    reward_mean = float(er.get(\"episode_return_mean\", float(\"nan\")))\n",
    "    len_mean    = float(er.get(\"episode_len_mean\", float(\"nan\")))\n",
    "    return reward_mean, len_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eval_env = FrogFly3DEnv(env_config)  # reuse your env_config\n",
    "\n",
    "\n",
    "def evaluate_policy(algo, env, num_episodes=5, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate a single-agent policy using the legacy RLlib API.\n",
    "\n",
    "    Uses algo.compute_single_action(obs, ...) which is the right call\n",
    "    for this RLlib version (compute_actions is multi-agent only here).\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    lengths = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Single-agent action: returns (action, state_out, info)\n",
    "            action, _, _ = algo.compute_single_action(obs, explore=False)\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            ep_ret += reward\n",
    "            steps += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        returns.append(ep_ret)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    return {\n",
    "        \"mean_return\": float(np.mean(returns)),\n",
    "        \"std_return\": float(np.std(returns)),\n",
    "        \"mean_length\": float(np.mean(lengths)),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_catch_rate(algo, env_config, episodes=50):\n",
    "    env = FrogFly3DEnv(env_config)\n",
    "    catches = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        for _ in range(env.max_steps):\n",
    "            action, _, _ = algo.compute_single_action(obs, explore=False)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            if terminated:\n",
    "                catches += 1\n",
    "                break\n",
    "            if truncated:\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    return catches / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'algo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# ---- 1. One PPO training iteration ----\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     15\u001b[0m     train_reward, train_len \u001b[38;5;241m=\u001b[39m get_train_metrics(result)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# ---- 2. Periodic evaluation ----\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'algo' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_iterations = 200        # how long you want to train\n",
    "eval_interval  = 10         # how often to run eval\n",
    "eval_episodes  = 5\n",
    "\n",
    "train_history = []          # for plotting later\n",
    "best_eval_return = -float(\"inf\")\n",
    "checkpoint_dir = \"frog_transformer_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, num_iterations + 1):\n",
    "    # ---- 1. One PPO training iteration ----\n",
    "    result = algo.train()\n",
    "    train_reward, train_len = get_train_metrics(result)\n",
    "\n",
    "    # ---- 2. Periodic evaluation ----\n",
    "    if i % eval_interval == 0:\n",
    "        eval_stats = evaluate_policy(\n",
    "            algo,\n",
    "            eval_env,\n",
    "            num_episodes=eval_episodes,\n",
    "            render=False,\n",
    "        )\n",
    "        eval_return = eval_stats[\"mean_return\"]\n",
    "        eval_len    = eval_stats[\"mean_length\"]\n",
    "\n",
    "        # NEW — compute catch rate\n",
    "        catch_rate = evaluate_catch_rate(algo, env_config, episodes=20)\n",
    "\n",
    "        print(\n",
    "            f\"[Iter {i:4d}] \"\n",
    "            f\"train_reward={train_reward:8.2f}, \"\n",
    "            f\"train_len={train_len:6.1f} | \"\n",
    "            f\"eval_return={eval_return:8.2f}, \"\n",
    "            f\"eval_len={eval_len:6.1f} | \"\n",
    "            f\"catch_rate={catch_rate*100:5.1f}%\"\n",
    "        )\n",
    "\n",
    "        train_history.append(\n",
    "            {\n",
    "                \"iter\": i,\n",
    "                \"train_reward\": train_reward,\n",
    "                \"train_len\": train_len,\n",
    "                \"eval_return\": eval_return,\n",
    "                \"eval_len\": eval_len,\n",
    "                \"catch_rate\": catch_rate,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # save best checkpoint\n",
    "        if eval_return > best_eval_return:\n",
    "            best_eval_return = eval_return\n",
    "            checkpoint_path = algo.save(checkpoint_dir)\n",
    "            print(f\"  New best eval_return={best_eval_return:.2f}, saved to {checkpoint_path}\")\n",
    "\n",
    "    else:\n",
    "        # Lighter log on non-eval iterations\n",
    "        print(\n",
    "            f\"[Iter {i:4d}] \"\n",
    "            f\"train_reward={train_reward:8.2f}, \"\n",
    "            f\"train_len={train_len:6.1f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df_hist \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_history)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mdf_hist\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, df_hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain reward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(df_hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m], df_hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_return\u001b[39m\u001b[38;5;124m\"\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval return\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-project/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-project/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'iter'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_hist = pd.DataFrame(train_history)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_hist[\"iter\"], df_hist[\"train_reward\"], label=\"train reward\")\n",
    "plt.plot(df_hist[\"iter\"], df_hist[\"eval_return\"], label=\"eval return\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Episode return\")\n",
    "plt.legend()\n",
    "plt.title(\"Frog Transformer PPO training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3D GIF to: transformer_original_3d.gif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-12-02 15:59:21,336 E 2877486 2877486] (raylet) node_manager.cc:3277: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: bb845cb40520f1b50aaa49aaa0b1c0f7d0c4ee4cc6af62ca1b229b4f, IP: 192.168.86.52) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.86.52`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import imageio\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. UNWRAP ENV (in case it's inside ObsHistoryWrapper)\n",
    "# ------------------------------------------------------------\n",
    "def unwrap_env(env):\n",
    "    \"\"\"Return underlying FrogFly3DEnv even if wrapped.\"\"\"\n",
    "    while hasattr(env, \"env\"):\n",
    "        env = env.env\n",
    "    return env\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. ROLLOUT AND RECORD POSITIONS\n",
    "# ------------------------------------------------------------\n",
    "def rollout_trajectory(env, algo, max_steps=200):\n",
    "    raw_env = unwrap_env(env)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    frog_traj = [raw_env.frog_pos.copy()]\n",
    "    fly_traj  = [raw_env.fly_pos.copy()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action, _, _ = algo.compute_single_action(obs, explore=False)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        frog_traj.append(raw_env.frog_pos.copy())\n",
    "        fly_traj.append(raw_env.fly_pos.copy())\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return np.array(frog_traj), np.array(fly_traj)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. CREATE 3D ANIMATION\n",
    "# ------------------------------------------------------------\n",
    "def make_3d_gif(frog_traj, fly_traj, gif_path=\"transformer_original_3d.gif\"):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # initial points\n",
    "    frog_dot, = ax.plot([], [], [], 'go', markersize=8)\n",
    "    fly_dot,  = ax.plot([], [], [], 'ro', markersize=8)\n",
    "\n",
    "    # history lines\n",
    "    frog_path, = ax.plot([], [], [], 'g-', linewidth=2, alpha=0.7)\n",
    "    fly_path,  = ax.plot([], [], [], 'r-', linewidth=2, alpha=0.7)\n",
    "\n",
    "    # axis limits\n",
    "    ax.set_xlim([-1.0, 1.0])\n",
    "    ax.set_ylim([-1.0, 1.0])\n",
    "    ax.set_zlim([-1.0, 1.0])\n",
    "    ax.set_title(\"Frog-Fly 3D Trajectory (Original Transformer PPO)\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "\n",
    "    def init():\n",
    "        frog_dot.set_data([], [])\n",
    "        frog_dot.set_3d_properties([])\n",
    "        fly_dot.set_data([], [])\n",
    "        fly_dot.set_3d_properties([])\n",
    "        frog_path.set_data([], [])\n",
    "        frog_path.set_3d_properties([])\n",
    "        fly_path.set_data([], [])\n",
    "        fly_path.set_3d_properties([])\n",
    "        return frog_dot, fly_dot, frog_path, fly_path\n",
    "\n",
    "    def update(i):\n",
    "        # --- frog point ---\n",
    "        frog_dot.set_data([frog_traj[i, 0]], [frog_traj[i, 1]])\n",
    "        frog_dot.set_3d_properties([frog_traj[i, 2]])\n",
    "\n",
    "        # --- fly point ---\n",
    "        fly_dot.set_data([fly_traj[i, 0]], [fly_traj[i, 1]])\n",
    "        fly_dot.set_3d_properties([fly_traj[i, 2]])\n",
    "\n",
    "        # --- frog path ---\n",
    "        frog_path.set_data(frog_traj[:i+1, 0], frog_traj[:i+1, 1])\n",
    "        frog_path.set_3d_properties(frog_traj[:i+1, 2])\n",
    "\n",
    "        # --- fly path ---\n",
    "        fly_path.set_data(fly_traj[:i+1, 0], fly_traj[:i+1, 1])\n",
    "        fly_path.set_3d_properties(fly_traj[:i+1, 2])\n",
    "\n",
    "        return frog_dot, fly_dot, frog_path, fly_path\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, init_func=init,\n",
    "        frames=len(frog_traj),\n",
    "        interval=60,\n",
    "        blit=False  # IMPORTANT for 3D\n",
    "    )\n",
    "\n",
    "    ani.save(gif_path, writer=\"pillow\", fps=20)\n",
    "\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved 3D GIF to: {gif_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. RUN IT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# STEP A — create a fresh evaluation env (unwrapped)\n",
    "eval_env = FrogFly3DEnv(env_config)\n",
    "\n",
    "# STEP B — rollout using the trained algorithm \"algo\"\n",
    "frog_traj, fly_traj = rollout_trajectory(eval_env, algo)\n",
    "\n",
    "# STEP C — render the GIF\n",
    "make_3d_gif(frog_traj, fly_traj, gif_path=\"transformer_original_3d.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered frog_hist_env OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=88185)\u001b[0m [2025-12-02 17:46:32,207 E 88185 88513] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "2025-12-02 17:47:24,444\tINFO worker.py:2023 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'resources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 47\u001b[0m\n\u001b[1;32m     42\u001b[0m     state \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# ---- DISABLE any Ray rollout workers ----\u001b[39;00m\n\u001b[1;32m     45\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rollout_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_env_on_local_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m(num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# ---- Build EMPTY algorithm object ----\u001b[39;00m\n\u001b[1;32m     50\u001b[0m algo \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbuild()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'resources'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-12-02 17:47:53,335 E 90158 90158] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-12-02 17:47:54,379 E 90258 90258] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(pid=90326)\u001b[0m [2025-12-02 17:47:55,225 E 90326 90400] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-12-02 17:47:55,475 E 51038 90319] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  FROG-FLY TRANSFORMER — DETERMINISTIC GIF GENERATION (10 runs)\n",
    "# ================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from matplotlib import animation\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# ASSUMPTIONS:\n",
    "# - FrogFly3DEnv, ObsHistoryWrapper, env_config, and the\n",
    "#   custom model frog_transformer_policy already exist in\n",
    "#   earlier notebook cells.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Re-register environment exactly as in training\n",
    "# -------------------------------------------------------\n",
    "def make_frog_hist_env(env_config):\n",
    "    return ObsHistoryWrapper(FrogFly3DEnv(env_config), K=24)\n",
    "\n",
    "register_env(\"frog_hist_env\", make_frog_hist_env)\n",
    "print(\"Registered frog_hist_env OK.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Clean Ray init + restore Algorithm state\n",
    "# -------------------------------------------------------\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# ---- Load saved PPO state ----\n",
    "with open(\"frog_transformer_checkpoints/algorithm_state.pkl\", \"rb\") as f:\n",
    "    state = pickle.load(f)\n",
    "\n",
    "# ---- DISABLE any Ray rollout workers ----\n",
    "config = config\\\n",
    "    .rollouts(num_rollout_workers=0, create_env_on_local_worker=False)\\\n",
    "    .resources(num_gpus=0)\n",
    "\n",
    "# ---- Build EMPTY algorithm object ----\n",
    "algo = config.build()\n",
    "\n",
    "# ---- Restore everything into that shell ----\n",
    "algo.__setstate__(state)\n",
    "\n",
    "print(\"Algorithm restored OK without rollout workers.\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Deterministic rollout\n",
    "# -------------------------------------------------------\n",
    "def rollout_once(algo, env_config, max_steps=200, seed=0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = make_frog_hist_env(env_config)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "\n",
    "    frog_traj, fly_traj = [], []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        frog_traj.append(env.env.frog_pos.copy())\n",
    "        fly_traj.append(env.env.fly_pos.copy())\n",
    "\n",
    "        action, _, _ = algo.compute_single_action(obs, explore=False)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    return np.array(frog_traj), np.array(fly_traj)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Make GIF\n",
    "# -------------------------------------------------------\n",
    "def make_gif(frog_traj, fly_traj, out_path):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_xlim([-1,1]); ax.set_ylim([-1,1]); ax.set_zlim([-1,1])\n",
    "    ax.set_title(\"Frog-Fly Transformer Policy Demo\")\n",
    "\n",
    "    frog_point, = ax.plot([], [], [], \"go\", markersize=8)\n",
    "    fly_point,  = ax.plot([], [], [], \"ro\", markersize=5)\n",
    "\n",
    "    def init():\n",
    "        frog_point.set_data([], [])\n",
    "        frog_point.set_3d_properties([])\n",
    "        fly_point.set_data([], [])\n",
    "        fly_point.set_3d_properties([])\n",
    "        return frog_point, fly_point\n",
    "\n",
    "    def update(i):\n",
    "        f  = frog_traj[i]\n",
    "        fl = fly_traj[i]\n",
    "        frog_point.set_data([f[0]], [f[1]])\n",
    "        frog_point.set_3d_properties([f[2]])\n",
    "        fly_point.set_data([fl[0]], [fl[1]])\n",
    "        fly_point.set_3d_properties([fl[2]])\n",
    "        return frog_point, fly_point\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update, init_func=init,\n",
    "        frames=len(frog_traj), interval=50, blit=True\n",
    "    )\n",
    "    ani.save(out_path, fps=30)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Generate 10 deterministic GIFs\n",
    "# -------------------------------------------------------\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "gif_folder = f\"frogfly_gifs_{timestamp}\"\n",
    "os.makedirs(gif_folder, exist_ok=True)\n",
    "print(\"Output folder:\", gif_folder)\n",
    "\n",
    "for i in range(10):\n",
    "    seed = i * 100\n",
    "    frog_traj, fly_traj = rollout_once(algo, env_config, seed=seed)\n",
    "    out_path = os.path.join(gif_folder, f\"demo_seed_{seed}.gif\")\n",
    "    make_gif(frog_traj, fly_traj, out_path)\n",
    "    print(f\"Saved GIF {i+1}/10  → {out_path}\")\n",
    "\n",
    "print(\"All GIFs complete →\", gif_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
