{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (needed for 3D)\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_3d(pos, num_freqs=4, max_freq=10.0):\n",
    "    \"\"\"\n",
    "    pos: np.array shape (3,) with (x, y, z) in [-space_size, space_size].\n",
    "    Returns: encoding vector of shape (3 * 2 * num_freqs,)\n",
    "    \"\"\"\n",
    "    pos = np.asarray(pos, dtype=np.float32)\n",
    "    assert pos.shape == (3,)\n",
    "\n",
    "    freqs = np.linspace(1.0, max_freq, num_freqs)\n",
    "    enc = []\n",
    "    for coord in pos:            # x, y, z\n",
    "        for f in freqs:\n",
    "            enc.append(np.sin(f * coord))\n",
    "            enc.append(np.cos(f * coord))\n",
    "    return np.array(enc, dtype=np.float32)\n",
    "\n",
    "\n",
    "class FrogFly3DEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    3D continuous environment:\n",
    "    - Frog (agent) moves to catch a moving fly.\n",
    "    - Observations: positions + relative vector + positional encodings.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            config = {}\n",
    "\n",
    "        self.space_size = float(config.get(\"space_size\", 1.0))\n",
    "        self.step_size = float(config.get(\"step_size\", 0.1))\n",
    "        self.fly_speed = float(config.get(\"fly_speed\", 0.05))\n",
    "        self.catch_radius = float(config.get(\"catch_radius\", 0.15))\n",
    "        self.max_steps = int(config.get(\"max_steps\", 200))\n",
    "        self.num_freqs = int(config.get(\"num_freqs\", 4))\n",
    "        self.use_positional_encodings = bool(\n",
    "            config.get(\"use_positional_encodings\", True)\n",
    "        )\n",
    "\n",
    "        # Action = 3D movement in [-1, 1]^3\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(3,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation components\n",
    "        # frog_pos (3) + fly_pos (3) + rel (3) = 9\n",
    "        base_dim = 9\n",
    "        if self.use_positional_encodings:\n",
    "            pe_dim = 3 * 2 * self.num_freqs  # per position\n",
    "            obs_dim = base_dim + 2 * pe_dim\n",
    "        else:\n",
    "            obs_dim = base_dim\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-5.0, high=5.0, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.frog_pos = None\n",
    "        self.fly_pos = None\n",
    "        self.steps = 0\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "\n",
    "    def _sample_position(self):\n",
    "        return np.random.uniform(\n",
    "            low=-self.space_size,\n",
    "            high=self.space_size,\n",
    "            size=(3,),\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def _clip_position(self, pos):\n",
    "        return np.clip(pos, -self.space_size, self.space_size).astype(np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        rel = self.fly_pos - self.frog_pos\n",
    "        parts = [self.frog_pos, self.fly_pos, rel]\n",
    "\n",
    "        if self.use_positional_encodings:\n",
    "            frog_pe = positional_encoding_3d(self.frog_pos, self.num_freqs)\n",
    "            fly_pe = positional_encoding_3d(self.fly_pos, self.num_freqs)\n",
    "            parts.extend([frog_pe, fly_pe])\n",
    "\n",
    "        return np.concatenate(parts, axis=0).astype(np.float32)\n",
    "\n",
    "    def _get_distance(self):\n",
    "        return float(np.linalg.norm(self.fly_pos - self.frog_pos))\n",
    "\n",
    "    # ---------- Gymnasium API ----------\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.frog_pos = self._sample_position()\n",
    "        self.fly_pos = self._sample_position()\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        action = np.asarray(action, dtype=np.float32)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        # Frog moves\n",
    "        self.frog_pos = self.frog_pos + self.step_size * action\n",
    "        self.frog_pos = self._clip_position(self.frog_pos)\n",
    "\n",
    "        # Fly moves randomly\n",
    "        fly_dir = np.random.normal(size=(3,)).astype(np.float32)\n",
    "        fly_dir /= np.linalg.norm(fly_dir) + 1e-8\n",
    "        self.fly_pos = self.fly_pos + self.fly_speed * fly_dir\n",
    "        self.fly_pos = self._clip_position(self.fly_pos)\n",
    "\n",
    "        dist = self._get_distance()\n",
    "        caught = dist < self.catch_radius\n",
    "\n",
    "        # Reward shaping: closer is better + bonus for catch\n",
    "        reward = -dist\n",
    "        if caught:\n",
    "            reward += 10.0\n",
    "\n",
    "        terminated = caught\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {\"distance\": dist, \"caught\": caught}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"Step {self.steps} | \"\n",
    "            f\"Frog: {self.frog_pos} | Fly: {self.fly_pos} | \"\n",
    "            f\"Dist: {self._get_distance():.3f}\"\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:54:15,720\tINFO worker.py:2023 -- Started a local Ray instance.\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "class FrogTransformerModel(TorchModelV2, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        **kwargs,  # ðŸ‘ˆ swallow extra kwargs RLlib might pass (like d_model)\n",
    "    ):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        # ---- Observation shape ----\n",
    "        # Use np.prod (np.product is deprecated in NumPy 2.x)\n",
    "        self.obs_dim = int(np.prod(obs_space.shape))\n",
    "\n",
    "        # ---- Hyperparameters ----\n",
    "        # Prefer explicit kwargs if RLlib passes them, otherwise fall back to custom_model_config.\n",
    "        cmc = model_config.get(\"custom_model_config\", {}) or {}\n",
    "\n",
    "        def get_hp(key, default):\n",
    "            return kwargs.get(key, cmc.get(key, default))\n",
    "\n",
    "        d_model      = get_hp(\"d_model\", 128)\n",
    "        nhead        = get_hp(\"nhead\", 4)\n",
    "        num_layers   = get_hp(\"num_layers\", 2)\n",
    "        dim_ff       = get_hp(\"dim_feedforward\", 256)\n",
    "        dropout      = get_hp(\"dropout\", 0.1)\n",
    "\n",
    "        self.seq_len = self.obs_dim\n",
    "\n",
    "        # ---- Embedding + Transformer ----\n",
    "        self.embed = nn.Linear(1, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # ---- Policy & value heads ----\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_outputs),\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "        self._features = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        x = input_dict[\"obs\"].float()        # (B, obs_dim)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Treat each scalar as a token: (B, T, 1)\n",
    "        x = x.view(B, self.seq_len, 1)\n",
    "        x = self.embed(x)                    # (B, T, d_model)\n",
    "        x = self.transformer(x)              # (B, T, d_model)\n",
    "\n",
    "        # Mean-pool tokens\n",
    "        features = x.mean(dim=1)             # (B, d_model)\n",
    "        self._features = features\n",
    "\n",
    "        logits = self.policy_head(features)  # (B, num_outputs)\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        assert self._features is not None\n",
    "        v = self.value_head(self._features)\n",
    "        return v.squeeze(-1)\n",
    "\n",
    "\n",
    "# â¬‡ï¸ keep this AFTER ray.init() in your setup cell\n",
    "ModelCatalog.register_custom_model(\"frog_transformer_policy\", FrogTransformerModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 18:54:16,429\tWARNING 2400300852.py:44 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-11-29 18:54:16,511 E 85505 63433819] core_worker.cc:2223: Actor with class name: 'RolloutWorker' and ID: 'ec6700d1291ad9eb2e1ad6c701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-11-29 18:54:19,298\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Algo built OK with Transformer policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-29 18:54:45,057 E 85509 63433906] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-29 18:54:45,672 E 85513 63434008] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(RolloutWorker pid=85518)\u001b[0m [2025-11-29 18:54:46,306 E 85518 63434290] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-29 18:54:46,437 E 85505 63434062] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_config = {\n",
    "    \"space_size\": 1.0,\n",
    "    \"step_size\": 0.1,\n",
    "    \"fly_speed\": 0.05,\n",
    "    \"catch_radius\": 0.15,\n",
    "    \"max_steps\": 200,\n",
    "    \"num_freqs\": 4,\n",
    "    \"use_positional_encodings\": True,\n",
    "}\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=FrogFly3DEnv, env_config=env_config)\n",
    "    .framework(\"torch\")\n",
    "    # âœ… Use the OLD API stack so `custom_model` works\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False,\n",
    "    )\n",
    "    # âœ… Explicitly say we want 1 env runner\n",
    "    .env_runners(\n",
    "        num_env_runners=1,\n",
    "        create_env_on_local_worker=True,\n",
    "    )\n",
    "    .resources(num_gpus=0)\n",
    "    .training(\n",
    "        model={\n",
    "            \"custom_model\": \"frog_transformer_policy\",\n",
    "            \"custom_model_config\": {\n",
    "                \"d_model\": 128,\n",
    "                \"nhead\": 4,\n",
    "                \"num_layers\": 2,\n",
    "                \"dim_feedforward\": 256,\n",
    "                \"dropout\": 0.1,\n",
    "            },\n",
    "        },\n",
    "        gamma=0.99,\n",
    "        lr=3e-4,\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(\"âœ… Algo built OK with Transformer policy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_policy(algo, env, num_episodes=5, render=False):\n",
    "    \"\"\"Run the current policy for a few episodes and return mean reward + mean length.\"\"\"\n",
    "    returns = []\n",
    "    lengths = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        episode_return = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # Deterministic action for evaluation\n",
    "            action = algo.compute_single_action(obs, explore=False)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_return += reward\n",
    "            steps += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        returns.append(episode_return)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    return {\n",
    "        \"mean_return\": float(np.mean(returns)),\n",
    "        \"std_return\": float(np.std(returns)),\n",
    "        \"mean_length\": float(np.mean(lengths)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['custom_metrics', 'episode_media', 'info', 'env_runners', 'num_healthy_workers', 'actor_manager_num_outstanding_async_reqs', 'num_remote_worker_restarts', 'num_agent_steps_sampled', 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'num_env_steps_sampled_throughput_per_sec', 'num_env_steps_trained_throughput_per_sec', 'timesteps_total', 'num_env_steps_sampled_lifetime', 'num_agent_steps_sampled_lifetime', 'num_steps_trained_this_iter', 'agent_timesteps_total', 'timers', 'counters', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore', 'perf'])\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float32(1.3348116), 'cur_kl_coeff': np.float64(0.20000000000000004), 'cur_lr': np.float64(0.0003), 'total_loss': np.float64(9.865064540473364), 'policy_loss': np.float64(0.012033222033892588), 'vf_loss': np.float64(9.851524303805444), 'vf_explained_var': np.float64(6.971788662736134e-05), 'kl': np.float64(0.007535005217998274), 'entropy': np.float64(4.286424523015176), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(128.0), 'num_grad_updates_lifetime': np.float64(2067.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(464.5)}}, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 8000}, 'env_runners': {'episode_reward_max': 8.623790241777897, 'episode_reward_min': -549.0518832206726, 'episode_reward_mean': np.float64(-336.5186970446442), 'episode_len_mean': np.float64(193.70491803278688), 'episode_media': {}, 'episodes_timesteps_total': 11816, 'policy_reward_min': {'default_policy': np.float64(-549.0518832206726)}, 'policy_reward_max': {'default_policy': np.float64(8.623790241777897)}, 'policy_reward_mean': {'default_policy': np.float64(-336.5186970446442)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-226.46265026926994, -260.13508731126785, -549.0518832206726, -135.06359446048737, -405.95097506046295, -175.5498480796814, -264.2572329044342, -395.37960839271545, -401.6808992624283, -365.3516511917114, -278.8401407599449, -352.8105424642563, -452.86177718639374, -291.00028920173645, -432.06135058403015, -300.14248180389404, -201.2544745504856, -294.6010618507862, -471.7249348163605, -431.9815876483917, -332.77960884571075, -435.0872576236725, -241.70647482573986, -346.3067846298218, -403.6103539466858, -371.9999499320984, -444.7487646341324, -319.9373047351837, -511.5978819131851, -386.978532910347, -163.75001022219658, -471.0365581512451, -446.1147227883339, -180.42686635255814, -495.65252232551575, -221.12666577100754, -465.25952368974686, -413.36303865909576, -369.84897565841675, 6.53962092846632, -393.0492924451828, -324.29780328273773, 8.623790241777897, -328.28482365608215, -390.4291228055954, -351.59445202350616, -392.34818708896637, -342.80670523643494, -344.2749574780464, -323.1707773208618, -282.2420619726181, -295.07894551754, -371.3744008541107, -429.73528122901917, -406.6085407733917, -358.2114554643631, -339.701385140419, -310.44604539871216, -303.53786939382553, -262.36819538474083, -285.74975979328156], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 10, 200, 200, 6, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_default_policy_reward': [-226.46265026926994, -260.13508731126785, -549.0518832206726, -135.06359446048737, -405.95097506046295, -175.5498480796814, -264.2572329044342, -395.37960839271545, -401.6808992624283, -365.3516511917114, -278.8401407599449, -352.8105424642563, -452.86177718639374, -291.00028920173645, -432.06135058403015, -300.14248180389404, -201.2544745504856, -294.6010618507862, -471.7249348163605, -431.9815876483917, -332.77960884571075, -435.0872576236725, -241.70647482573986, -346.3067846298218, -403.6103539466858, -371.9999499320984, -444.7487646341324, -319.9373047351837, -511.5978819131851, -386.978532910347, -163.75001022219658, -471.0365581512451, -446.1147227883339, -180.42686635255814, -495.65252232551575, -221.12666577100754, -465.25952368974686, -413.36303865909576, -369.84897565841675, 6.53962092846632, -393.0492924451828, -324.29780328273773, 8.623790241777897, -328.28482365608215, -390.4291228055954, -351.59445202350616, -392.34818708896637, -342.80670523643494, -344.2749574780464, -323.1707773208618, -282.2420619726181, -295.07894551754, -371.3744008541107, -429.73528122901917, -406.6085407733917, -358.2114554643631, -339.701385140419, -310.44604539871216, -303.53786939382553, -262.36819538474083, -285.74975979328156]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.11283640547884807), 'mean_inference_ms': np.float64(0.750253518401932), 'mean_action_processing_ms': np.float64(0.055760744043187024), 'mean_env_wait_ms': np.float64(0.07019198494921002), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.002567103651703381), 'StateBufferConnector_ms': np.float64(0.0018096361003938268), 'ViewRequirementAgentConnector_ms': np.float64(0.04339413564713275)}, 'num_episodes': 21, 'episode_return_max': 8.623790241777897, 'episode_return_min': -549.0518832206726, 'episode_return_mean': np.float64(-336.5186970446442), 'episodes_this_iter': 21}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 8000, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 8000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 36.92235545828893, 'num_env_steps_trained_throughput_per_sec': 36.92235545828893, 'timesteps_total': 12000, 'num_env_steps_sampled_lifetime': 12000, 'num_agent_steps_sampled_lifetime': 12000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 12000, 'timers': {'training_iteration_time_ms': 99740.003, 'restore_workers_time_ms': 0.12, 'training_step_time_ms': 99739.56, 'sample_time_ms': 3975.88, 'load_time_ms': 0.557, 'load_throughput': 4787335.141, 'learn_time_ms': 95752.669, 'learn_throughput': 27.85, 'synch_weights_time_ms': 14.538}, 'counters': {'num_env_steps_sampled': 12000, 'num_env_steps_trained': 8000, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 8000}, 'done': False, 'training_iteration': 2, 'trial_id': 'default', 'date': '2025-11-29_19-02-17', 'timestamp': 1764460937, 'time_this_iter_s': 108.35961699485779, 'time_total_s': 217.660297870636, 'pid': 85505, 'hostname': 'mac.lan', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': <class '__main__.FrogFly3DEnv'>, 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0003, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 128, 'nhead': 4, 'num_layers': 2, 'dim_feedforward': 256, 'dropout': 0.1}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x305b45750>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 217.660297870636, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': np.float64(24.767719298245616), 'ram_util_percent': np.float64(80.26070175438596)}}\n"
     ]
    }
   ],
   "source": [
    "res = algo.train()\n",
    "print(res.keys())\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: reward_mean=-317.19, len_mean=190.0\n",
      "Iter 2: reward_mean=-308.15, len_mean=188.6\n",
      "Iter 3: reward_mean=-296.04, len_mean=187.2\n",
      "Iter 4: reward_mean=-287.72, len_mean=189.9\n",
      "Iter 5: reward_mean=-282.44, len_mean=189.9\n",
      "Iter 6: reward_mean=-295.29, len_mean=195.5\n",
      "Iter 7: reward_mean=-300.70, len_mean=197.4\n",
      "Iter 8: reward_mean=-298.41, len_mean=198.8\n",
      "Iter 9: reward_mean=-296.09, len_mean=197.6\n",
      "Iter 10: reward_mean=-292.10, len_mean=193.7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "\n",
    "    er = result[\"env_runners\"]  # shorthand\n",
    "\n",
    "    reward_mean = er[\"episode_return_mean\"]\n",
    "    len_mean    = er[\"episode_len_mean\"]\n",
    "\n",
    "    print(\n",
    "        f\"Iter {i+1}: \"\n",
    "        f\"reward_mean={float(reward_mean):.2f}, \"\n",
    "        f\"len_mean={float(len_mean):.1f}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
