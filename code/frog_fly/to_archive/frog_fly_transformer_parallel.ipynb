{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 (needed for 3D)\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding_3d(pos, num_freqs=4, max_freq=10.0):\n",
    "    \"\"\"\n",
    "    pos: np.array shape (3,) with (x, y, z) in [-space_size, space_size].\n",
    "    Returns: encoding vector of shape (3 * 2 * num_freqs,)\n",
    "    \"\"\"\n",
    "    pos = np.asarray(pos, dtype=np.float32)\n",
    "    assert pos.shape == (3,)\n",
    "\n",
    "    freqs = np.linspace(1.0, max_freq, num_freqs)\n",
    "    enc = []\n",
    "    for coord in pos:            # x, y, z\n",
    "        for f in freqs:\n",
    "            enc.append(np.sin(f * coord))\n",
    "            enc.append(np.cos(f * coord))\n",
    "    return np.array(enc, dtype=np.float32)\n",
    "\n",
    "\n",
    "class FrogFly3DEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    3D continuous environment:\n",
    "    - Frog (agent) moves to catch a moving fly.\n",
    "    - Observations: positions + relative vector + positional encodings.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            config = {}\n",
    "\n",
    "        self.space_size = float(config.get(\"space_size\", 1.0))\n",
    "        self.step_size = float(config.get(\"step_size\", 0.1))\n",
    "        self.fly_speed = float(config.get(\"fly_speed\", 0.05))\n",
    "        self.catch_radius = float(config.get(\"catch_radius\", 0.15))\n",
    "        self.max_steps = int(config.get(\"max_steps\", 200))\n",
    "        self.num_freqs = int(config.get(\"num_freqs\", 4))\n",
    "        self.use_positional_encodings = bool(\n",
    "            config.get(\"use_positional_encodings\", True)\n",
    "        )\n",
    "\n",
    "        # Action = 3D movement in [-1, 1]^3\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(3,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Observation components\n",
    "        # frog_pos (3) + fly_pos (3) + rel (3) = 9\n",
    "        base_dim = 9\n",
    "        if self.use_positional_encodings:\n",
    "            pe_dim = 3 * 2 * self.num_freqs  # per position\n",
    "            obs_dim = base_dim + 2 * pe_dim\n",
    "        else:\n",
    "            obs_dim = base_dim\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-5.0, high=5.0, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.frog_pos = None\n",
    "        self.fly_pos = None\n",
    "        self.steps = 0\n",
    "\n",
    "    # ---------- Helpers ----------\n",
    "\n",
    "    def _sample_position(self):\n",
    "        return np.random.uniform(\n",
    "            low=-self.space_size,\n",
    "            high=self.space_size,\n",
    "            size=(3,),\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def _clip_position(self, pos):\n",
    "        return np.clip(pos, -self.space_size, self.space_size).astype(np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        rel = self.fly_pos - self.frog_pos\n",
    "        parts = [self.frog_pos, self.fly_pos, rel]\n",
    "\n",
    "        if self.use_positional_encodings:\n",
    "            frog_pe = positional_encoding_3d(self.frog_pos, self.num_freqs)\n",
    "            fly_pe = positional_encoding_3d(self.fly_pos, self.num_freqs)\n",
    "            parts.extend([frog_pe, fly_pe])\n",
    "\n",
    "        return np.concatenate(parts, axis=0).astype(np.float32)\n",
    "\n",
    "    def _get_distance(self):\n",
    "        return float(np.linalg.norm(self.fly_pos - self.frog_pos))\n",
    "\n",
    "    # ---------- Gymnasium API ----------\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.steps = 0\n",
    "        self.frog_pos = self._sample_position()\n",
    "        self.fly_pos = self._sample_position()\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "\n",
    "        action = np.asarray(action, dtype=np.float32)\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "\n",
    "        # Frog moves\n",
    "        self.frog_pos = self.frog_pos + self.step_size * action\n",
    "        self.frog_pos = self._clip_position(self.frog_pos)\n",
    "\n",
    "        # Fly moves randomly\n",
    "        fly_dir = np.random.normal(size=(3,)).astype(np.float32)\n",
    "        fly_dir /= np.linalg.norm(fly_dir) + 1e-8\n",
    "        self.fly_pos = self.fly_pos + self.fly_speed * fly_dir\n",
    "        self.fly_pos = self._clip_position(self.fly_pos)\n",
    "\n",
    "        dist = self._get_distance()\n",
    "        caught = dist < self.catch_radius\n",
    "\n",
    "        # Reward shaping: closer is better + bonus for catch\n",
    "        reward = -dist\n",
    "        if caught:\n",
    "            reward += 10.0\n",
    "\n",
    "        terminated = caught\n",
    "        truncated = self.steps >= self.max_steps\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {\"distance\": dist, \"caught\": caught}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        print(\n",
    "            f\"Step {self.steps} | \"\n",
    "            f\"Frog: {self.frog_pos} | Fly: {self.fly_pos} | \"\n",
    "            f\"Dist: {self._get_distance():.3f}\"\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:44:47,236\tINFO worker.py:2023 -- Started a local Ray instance.\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/_private/worker.py:2062: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "class FrogTransformerModel(TorchModelV2, nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        **kwargs,  # ðŸ‘ˆ swallow extra kwargs RLlib might pass (like d_model)\n",
    "    ):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        # ---- Observation shape ----\n",
    "        self.obs_dim = int(np.prod(obs_space.shape))\n",
    "\n",
    "        # ---- Hyperparameters ----\n",
    "        cmc = model_config.get(\"custom_model_config\", {}) or {}\n",
    "\n",
    "        def get_hp(key, default):\n",
    "            return kwargs.get(key, cmc.get(key, default))\n",
    "\n",
    "        d_model      = get_hp(\"d_model\", 128)\n",
    "        nhead        = get_hp(\"nhead\", 4)\n",
    "        num_layers   = get_hp(\"num_layers\", 2)\n",
    "        dim_ff       = get_hp(\"dim_feedforward\", 256)\n",
    "        dropout      = get_hp(\"dropout\", 0.1)\n",
    "\n",
    "        self.seq_len = self.obs_dim\n",
    "\n",
    "        # ---- Embedding + Transformer ----\n",
    "        self.embed = nn.Linear(1, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # ---- Policy & value heads ----\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_outputs),\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "        self._features = None\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        x = input_dict[\"obs\"].float()        # (B, obs_dim)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # Treat each scalar as a token: (B, T, 1)\n",
    "        x = x.view(B, self.seq_len, 1)\n",
    "        x = self.embed(x)                    # (B, T, d_model)\n",
    "        x = self.transformer(x)              # (B, T, d_model)\n",
    "\n",
    "        # Mean-pool tokens\n",
    "        features = x.mean(dim=1)             # (B, d_model)\n",
    "\n",
    "        # ðŸ”’ Guard against NaNs/Infs in features\n",
    "        features = torch.nan_to_num(features, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "\n",
    "        self._features = features\n",
    "\n",
    "        logits = self.policy_head(features)  # (B, num_outputs)\n",
    "\n",
    "        # ðŸ”’ Guard against NaNs/Infs in logits\n",
    "        logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def value_function(self):\n",
    "        assert self._features is not None\n",
    "        v = self.value_head(self._features)\n",
    "        # ðŸ”’ Guard again, just in case\n",
    "        v = torch.nan_to_num(v, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "        return v.squeeze(-1)\n",
    "\n",
    "\n",
    "ModelCatalog.register_custom_model(\"frog_transformer_policy\", FrogTransformerModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:44:47,987\tWARNING 728841178.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-11-29 19:44:48,063 E 87256 63478206] core_worker.cc:2223: Actor with class name: 'RolloutWorker' and ID: 'e083686760e20fb7315d82c201000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-11-29 19:44:50,997\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Algo built OK with *faster* Transformer PPO config\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "env_config = {\n",
    "    \"space_size\": 1.0,\n",
    "    \"step_size\": 0.1,\n",
    "    \"fly_speed\": 0.05,\n",
    "    \"catch_radius\": 0.15,\n",
    "    \"max_steps\": 200,\n",
    "    \"num_freqs\": 4,\n",
    "    \"use_positional_encodings\": True,\n",
    "}\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=FrogFly3DEnv, env_config=env_config)\n",
    "    .framework(\"torch\")\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=False,\n",
    "        enable_env_runner_and_connector_v2=False,\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=1,\n",
    "        num_envs_per_env_runner=4,\n",
    "        create_env_on_local_worker=True,\n",
    "    )\n",
    "    .resources(num_gpus=0)\n",
    "    .training(\n",
    "        model={\n",
    "            \"custom_model\": \"frog_transformer_policy\",\n",
    "            \"custom_model_config\": {\n",
    "                \"d_model\": 64,          # smaller = safer/faster\n",
    "                \"nhead\": 4,\n",
    "                \"num_layers\": 1,\n",
    "                \"dim_feedforward\": 128,\n",
    "                \"dropout\": 0.1,\n",
    "            },\n",
    "        },\n",
    "        gamma=0.99,\n",
    "        lr=1e-4,           # ðŸ”½ lower LR\n",
    "        grad_clip=0.5,     # ðŸ”’ global grad norm clip\n",
    "\n",
    "        train_batch_size=2000,\n",
    "        num_epochs=10,\n",
    "        minibatch_size=256,\n",
    "        clip_param=0.2,\n",
    "        vf_clip_param=10.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(\"âœ… Algo built OK with *faster* Transformer PPO config\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:44:51,740\tWARNING train_ops.py:114 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['custom_metrics', 'episode_media', 'info', 'env_runners', 'num_healthy_workers', 'actor_manager_num_outstanding_async_reqs', 'num_remote_worker_restarts', 'num_agent_steps_sampled', 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'num_env_steps_sampled_throughput_per_sec', 'num_env_steps_trained_throughput_per_sec', 'timesteps_total', 'num_env_steps_sampled_lifetime', 'num_agent_steps_sampled_lifetime', 'num_steps_trained_this_iter', 'agent_timesteps_total', 'timers', 'counters', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'config', 'time_since_restore', 'iterations_since_restore', 'perf'])\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.49946922063827515), 'cur_kl_coeff': np.float64(0.19999999999999993), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.901415824890137), 'policy_loss': np.float64(0.06651037079947335), 'vf_loss': np.float64(9.822513062613352), 'vf_explained_var': np.float64(-0.00019620742116655622), 'kl': np.float64(0.061961050467964796), 'entropy': np.float64(4.085609633581979), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(35.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'env_runners': {'episode_reward_max': 4.7353919595479965, 'episode_reward_min': -319.266759455204, 'episode_reward_mean': np.float64(-220.95723575055598), 'episode_len_mean': np.float64(169.3), 'episode_media': {}, 'episodes_timesteps_total': 1693, 'policy_reward_min': {'default_policy': np.float64(-319.266759455204)}, 'policy_reward_max': {'default_policy': np.float64(4.7353919595479965)}, 'policy_reward_mean': {'default_policy': np.float64(-220.95723575055598)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-283.0288296341896, -248.76995357871056, -297.7736083269119, -236.47350227832794, -315.084679543972, -319.266759455204, -217.4140323996544, -268.94528618454933, 4.7353919595479965, -27.551098063588142], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 200, 16, 77], 'policy_default_policy_reward': [-283.0288296341896, -248.76995357871056, -297.7736083269119, -236.47350227832794, -315.084679543972, -319.266759455204, -217.4140323996544, -268.94528618454933, 4.7353919595479965, -27.551098063588142]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.38518781909447697), 'mean_inference_ms': np.float64(0.6702660086626064), 'mean_action_processing_ms': np.float64(0.16775530969311375), 'mean_env_wait_ms': np.float64(0.2209245563743119), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.002613067626953125), 'StateBufferConnector_ms': np.float64(0.006852149963378906), 'ViewRequirementAgentConnector_ms': np.float64(0.033125877380371094)}, 'num_episodes': 10, 'episode_return_max': 4.7353919595479965, 'episode_return_min': -319.266759455204, 'episode_return_mean': np.float64(-220.95723575055598), 'episodes_this_iter': 10}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000, 'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 345.1301156904371, 'num_env_steps_trained_throughput_per_sec': 345.1301156904371, 'timesteps_total': 2000, 'num_env_steps_sampled_lifetime': 2000, 'num_agent_steps_sampled_lifetime': 2000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 2000, 'timers': {'training_iteration_time_ms': 5795.01, 'restore_workers_time_ms': 0.013, 'training_step_time_ms': 5794.88, 'sample_time_ms': 734.088, 'load_time_ms': 0.158, 'load_throughput': 12671613.293, 'learn_time_ms': 5057.431, 'learn_throughput': 395.458, 'synch_weights_time_ms': 2.52}, 'counters': {'num_env_steps_sampled': 2000, 'num_env_steps_trained': 2000, 'num_agent_steps_sampled': 2000, 'num_agent_steps_trained': 2000}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-11-29_19-44-56', 'timestamp': 1764463496, 'time_this_iter_s': 5.803385019302368, 'time_total_s': 5.803385019302368, 'pid': 87256, 'hostname': 'mac.lan', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': <class '__main__.FrogFly3DEnv'>, 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x17eca1750>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 5.803385019302368, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(24.666666666666668), 'ram_util_percent': np.float64(81.25555555555555)}}\n"
     ]
    }
   ],
   "source": [
    "res = algo.train()\n",
    "print(res.keys())\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: reward_mean=-294.40, len_mean=185.4\n",
      "Iter 2: reward_mean=-283.53, len_mean=189.8\n",
      "Iter 3: reward_mean=-295.41, len_mean=192.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=gcs_server)\u001b[0m [2025-11-29 19:45:16,564 E 87267 63478295] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[33m(raylet)\u001b[0m [2025-11-29 19:45:17,187 E 87271 63478397] (raylet) main.cc:979: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "\u001b[36m(RolloutWorker pid=87275)\u001b[0m [2025-11-29 19:45:17,872 E 87275 63478689] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n",
      "[2025-11-29 19:45:17,991 E 87256 63478447] core_worker_process.cc:837: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 4: reward_mean=-293.61, len_mean=191.7\n",
      "Iter 5: reward_mean=-301.06, len_mean=193.3\n",
      "Iter 6: reward_mean=-294.96, len_mean=189.5\n",
      "Iter 7: reward_mean=-297.88, len_mean=190.8\n",
      "Iter 8: reward_mean=-296.90, len_mean=190.9\n",
      "Iter 9: reward_mean=-295.01, len_mean=191.3\n",
      "Iter 10: reward_mean=-289.61, len_mean=189.9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "\n",
    "    er = result[\"env_runners\"]  # shorthand\n",
    "\n",
    "    reward_mean = er[\"episode_return_mean\"]\n",
    "    len_mean    = er[\"episode_len_mean\"]\n",
    "\n",
    "    print(\n",
    "        f\"Iter {i+1}: \"\n",
    "        f\"reward_mean={float(reward_mean):.2f}, \"\n",
    "        f\"len_mean={float(len_mean):.1f}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_metrics(result):\n",
    "    \"\"\"Extract mean reward and len from an RLlib result dict (env_runners layout).\"\"\"\n",
    "    er = result.get(\"env_runners\", {})\n",
    "    reward_mean = float(er.get(\"episode_return_mean\", float(\"nan\")))\n",
    "    len_mean    = float(er.get(\"episode_len_mean\", float(\"nan\")))\n",
    "    return reward_mean, len_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eval_env = FrogFly3DEnv(env_config)  # reuse your env_config\n",
    "\n",
    "\n",
    "def evaluate_policy(algo, env, num_episodes=5, render=False):\n",
    "    \"\"\"Run the current policy for a few episodes and return summary stats.\"\"\"\n",
    "    returns = []\n",
    "    lengths = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        ep_ret = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = algo.compute_single_action(obs, explore=False)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += reward\n",
    "            steps += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        returns.append(ep_ret)\n",
    "        lengths.append(steps)\n",
    "\n",
    "    return {\n",
    "        \"mean_return\": float(np.mean(returns)),\n",
    "        \"std_return\": float(np.std(returns)),\n",
    "        \"mean_length\": float(np.mean(lengths)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter    1] train_reward= -284.55, train_len= 189.9\n",
      "[Iter    2] train_reward= -281.69, train_len= 189.9\n",
      "[Iter    3] train_reward= -284.21, train_len= 191.0\n",
      "[Iter    4] train_reward= -282.73, train_len= 189.1\n",
      "[Iter    5] train_reward= -281.21, train_len= 190.8\n",
      "[Iter    6] train_reward= -284.46, train_len= 192.5\n",
      "[Iter    7] train_reward= -285.91, train_len= 193.3\n",
      "[Iter    8] train_reward= -286.58, train_len= 192.3\n",
      "[Iter    9] train_reward= -290.79, train_len= 191.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 19:46:56,126\tWARNING 1069323533.py:18 -- DeprecationWarning: `compute_single_action` has been deprecated. `Algorithm.compute_single_action` should no longer be used. Get the RLModule instance through `Algorithm.get_module([module ID])`, then compute actions through `RLModule.forward_inference({'obs': [obs batch]})`. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter   10] train_reward= -299.38, train_len= 195.7, eval_return= -263.32 Â±  75.20, eval_len= 200.0\n",
      "  âœ… New best eval_return=-263.32, saved to TrainingResult(checkpoint=Checkpoint(filesystem=local, path=frog_transformer_checkpoints), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(1.8310546875000006e-05), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.696848460606166), 'policy_loss': np.float64(-0.006743850878306798), 'vf_loss': np.float64(9.703592232295446), 'vf_explained_var': np.float64(0.00020001275198800223), 'kl': np.float64(0.003994126659901147), 'entropy': np.float64(3.438131822858538), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(1435.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 42000, 'num_env_steps_trained': 42000, 'num_agent_steps_sampled': 42000, 'num_agent_steps_trained': 42000}, 'env_runners': {'episode_reward_max': 5.331298775970936, 'episode_reward_min': -480.38624572753906, 'episode_reward_mean': np.float64(-299.383909002766), 'episode_len_mean': np.float64(195.66), 'episode_media': {}, 'episodes_timesteps_total': 19566, 'policy_reward_min': {'default_policy': np.float64(-480.38624572753906)}, 'policy_reward_max': {'default_policy': np.float64(5.331298775970936)}, 'policy_reward_mean': {'default_policy': np.float64(-299.383909002766)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-327.4040431380272, -266.86365634202957, -314.45013439655304, -344.246231675148, -207.27529859542847, -123.91625966131687, -188.0545263439417, -347.84115368127823, -218.4049610197544, -207.84586057066917, -329.21588242053986, -390.56681180000305, -401.07554292678833, -321.981032371521, -164.28838792443275, -369.8140745162964, -152.12737894058228, -316.3330384492874, -401.9344288110733, -349.34734749794006, -133.99775210022926, -266.70632442831993, -411.1740320920944, -200.09494298696518, -386.1124551296234, -246.58978924155235, -286.17138600349426, -340.05193066596985, -480.38624572753906, -94.56511478126049, -239.45603001117706, 5.331298775970936, -314.8148933649063, -267.94843858480453, -426.6884981393814, -404.8922109603882, -307.83209466934204, -236.45430743694305, -475.39749336242676, -406.0595237016678, -313.7058628797531, -309.66251008212566, -195.9205557703972, -294.8903483748436, -414.6087188720703, -231.1770259141922, -333.001690864563, -226.13809770345688, -276.7399955391884, -334.50921654701233, -337.7495583295822, -438.6639150381088, -270.5109032392502, -304.5208991765976, -242.07371509075165, -289.44043362140656, -266.1762928366661, -268.30331844091415, -289.52000814676285, -453.68814396858215, -187.7357083261013, -361.56372970342636, -250.96956384181976, -349.61344826221466, -263.8073136806488, -340.80799573659897, -245.37267205119133, -421.43683755397797, -389.70465886592865, -322.7640705704689, -316.0124555826187, -273.5090546011925, -444.4172109365463, -419.12239611148834, -353.86297476291656, -211.2910304069519, -285.61266404390335, -211.26245173811913, -36.925526797771454, -201.854778110981, -307.6690855026245, -249.64533925056458, -363.781218290329, -176.5095779299736, -310.46731120347977, -69.35817462205887, -398.99412858486176, -420.4723905324936, -382.6487511396408, -414.0524045228958, -322.18219900131226, -460.58973920345306, -218.3876765370369, -288.44929069280624, -237.8811686038971, -327.92447143793106, -267.059905230999, -377.9219117164612, -315.7829323410988, -386.91725611686707], 'episode_lengths': [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 13, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 74, 200, 200, 200, 200, 200, 200, 79, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_default_policy_reward': [-327.4040431380272, -266.86365634202957, -314.45013439655304, -344.246231675148, -207.27529859542847, -123.91625966131687, -188.0545263439417, -347.84115368127823, -218.4049610197544, -207.84586057066917, -329.21588242053986, -390.56681180000305, -401.07554292678833, -321.981032371521, -164.28838792443275, -369.8140745162964, -152.12737894058228, -316.3330384492874, -401.9344288110733, -349.34734749794006, -133.99775210022926, -266.70632442831993, -411.1740320920944, -200.09494298696518, -386.1124551296234, -246.58978924155235, -286.17138600349426, -340.05193066596985, -480.38624572753906, -94.56511478126049, -239.45603001117706, 5.331298775970936, -314.8148933649063, -267.94843858480453, -426.6884981393814, -404.8922109603882, -307.83209466934204, -236.45430743694305, -475.39749336242676, -406.0595237016678, -313.7058628797531, -309.66251008212566, -195.9205557703972, -294.8903483748436, -414.6087188720703, -231.1770259141922, -333.001690864563, -226.13809770345688, -276.7399955391884, -334.50921654701233, -337.7495583295822, -438.6639150381088, -270.5109032392502, -304.5208991765976, -242.07371509075165, -289.44043362140656, -266.1762928366661, -268.30331844091415, -289.52000814676285, -453.68814396858215, -187.7357083261013, -361.56372970342636, -250.96956384181976, -349.61344826221466, -263.8073136806488, -340.80799573659897, -245.37267205119133, -421.43683755397797, -389.70465886592865, -322.7640705704689, -316.0124555826187, -273.5090546011925, -444.4172109365463, -419.12239611148834, -353.86297476291656, -211.2910304069519, -285.61266404390335, -211.26245173811913, -36.925526797771454, -201.854778110981, -307.6690855026245, -249.64533925056458, -363.781218290329, -176.5095779299736, -310.46731120347977, -69.35817462205887, -398.99412858486176, -420.4723905324936, -382.6487511396408, -414.0524045228958, -322.18219900131226, -460.58973920345306, -218.3876765370369, -288.44929069280624, -237.8811686038971, -327.92447143793106, -267.059905230999, -377.9219117164612, -315.7829323410988, -386.91725611686707]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.3940488192303878), 'mean_inference_ms': np.float64(0.6819204724776496), 'mean_action_processing_ms': np.float64(0.16506512870704185), 'mean_env_wait_ms': np.float64(0.22223440690204757), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.001842498779296875), 'StateBufferConnector_ms': np.float64(0.0014104843139648438), 'ViewRequirementAgentConnector_ms': np.float64(0.033025503158569336)}, 'num_episodes': 10, 'episode_return_max': 5.331298775970936, 'episode_return_min': -480.38624572753906, 'episode_return_mean': np.float64(-299.383909002766), 'episodes_this_iter': 10}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 42000, 'num_agent_steps_trained': 42000, 'num_env_steps_sampled': 42000, 'num_env_steps_trained': 42000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 305.56639846671277, 'num_env_steps_trained_throughput_per_sec': 305.56639846671277, 'timesteps_total': 42000, 'num_env_steps_sampled_lifetime': 42000, 'num_agent_steps_sampled_lifetime': 42000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 42000, 'timers': {'training_iteration_time_ms': 6131.452, 'restore_workers_time_ms': 0.012, 'training_step_time_ms': 6131.412, 'sample_time_ms': 761.189, 'load_time_ms': 0.283, 'load_throughput': 7064090.947, 'learn_time_ms': 5366.352, 'learn_throughput': 372.693, 'synch_weights_time_ms': 3.415}, 'counters': {'num_env_steps_sampled': 42000, 'num_env_steps_trained': 42000, 'num_agent_steps_sampled': 42000, 'num_agent_steps_trained': 42000}, 'done': False, 'training_iteration': 21, 'trial_id': 'default', 'date': '2025-11-29_19-46-56', 'timestamp': 1764463616, 'time_this_iter_s': 6.553107023239136, 'time_total_s': 124.8141405582428, 'pid': 87256, 'hostname': 'mac.lan', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': <class '__main__.FrogFly3DEnv'>, 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x17eca1750>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 124.8141405582428, 'iterations_since_restore': 21, 'perf': {'cpu_util_percent': np.float64(40.62222222222222), 'ram_util_percent': np.float64(82.1)}})\n",
      "[Iter   11] train_reward= -306.20, train_len= 195.7\n",
      "[Iter   12] train_reward= -305.65, train_len= 195.7\n",
      "[Iter   13] train_reward= -305.73, train_len= 194.2\n",
      "[Iter   14] train_reward= -306.47, train_len= 196.1\n",
      "[Iter   15] train_reward= -311.56, train_len= 196.1\n",
      "[Iter   16] train_reward= -308.19, train_len= 196.1\n",
      "[Iter   17] train_reward= -309.83, train_len= 196.1\n",
      "[Iter   18] train_reward= -310.10, train_len= 195.7\n",
      "[Iter   19] train_reward= -307.43, train_len= 196.3\n",
      "[Iter   20] train_reward= -304.29, train_len= 193.7, eval_return= -365.49 Â±  39.56, eval_len= 200.0\n",
      "[Iter   21] train_reward= -297.56, train_len= 191.7\n",
      "[Iter   22] train_reward= -294.73, train_len= 191.7\n",
      "[Iter   23] train_reward= -297.45, train_len= 189.3\n",
      "[Iter   24] train_reward= -295.67, train_len= 189.3\n",
      "[Iter   25] train_reward= -297.64, train_len= 189.3\n",
      "[Iter   26] train_reward= -302.31, train_len= 189.3\n",
      "[Iter   27] train_reward= -307.35, train_len= 190.9\n",
      "[Iter   28] train_reward= -313.52, train_len= 191.6\n",
      "[Iter   29] train_reward= -314.17, train_len= 191.6\n",
      "[Iter   30] train_reward= -323.07, train_len= 194.1, eval_return= -195.08 Â± 116.04, eval_len= 161.4\n",
      "  âœ… New best eval_return=-195.08, saved to TrainingResult(checkpoint=Checkpoint(filesystem=local, path=frog_transformer_checkpoints), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': np.float64(0.0), 'grad_gnorm': np.float64(0.5), 'cur_kl_coeff': np.float64(1.7881393432617193e-08), 'cur_lr': np.float64(0.00010000000000000005), 'total_loss': np.float64(9.034654460634505), 'policy_loss': np.float64(0.08063789563519615), 'vf_loss': np.float64(8.954016556058612), 'vf_explained_var': np.float64(-0.0021315974848611016), 'kl': np.float64(0.0048208933772652274), 'entropy': np.float64(2.796545137677874), 'entropy_coeff': np.float64(0.0)}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': np.float64(256.0), 'num_grad_updates_lifetime': np.float64(2835.5), 'diff_num_grad_updates_vs_sampler_policy': np.float64(34.5)}}, 'num_env_steps_sampled': 82000, 'num_env_steps_trained': 82000, 'num_agent_steps_sampled': 82000, 'num_agent_steps_trained': 82000}, 'env_runners': {'episode_reward_max': 9.928175829350948, 'episode_reward_min': -519.7159857749939, 'episode_reward_mean': np.float64(-323.06744350470603), 'episode_len_mean': np.float64(194.12), 'episode_media': {}, 'episodes_timesteps_total': 19412, 'policy_reward_min': {'default_policy': np.float64(-519.7159857749939)}, 'policy_reward_max': {'default_policy': np.float64(9.928175829350948)}, 'policy_reward_mean': {'default_policy': np.float64(-323.06744350470603)}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-15.698133781552315, -153.3492911607027, -450.6548002958298, -390.75946855545044, -62.55089320987463, -191.14457085728645, -389.36784744262695, -309.03702062368393, -318.5986790060997, -359.6790996789932, -238.84544336795807, -394.66781079769135, -381.6682622432709, -287.2933521270752, -217.4937076717615, -179.0432384610176, -380.0932892560959, -402.2531422972679, -409.27451610565186, -343.84083300828934, -364.81315791606903, -271.6467836499214, 6.77240813523531, -420.44991636276245, -322.57012808322906, 9.928175829350948, -296.3358725309372, -229.9737567305565, -312.4900425672531, -471.0608569383621, -358.43216490745544, -338.26797235012054, -287.8806974887848, -432.964985370636, -153.07981809973717, -459.13139259815216, -297.03564727306366, -253.60449689626694, -349.8328113555908, -370.17685198783875, -408.9625543355942, -267.2017743587494, -401.6510409116745, -348.58081609010696, -179.7777094244957, -314.5466828942299, -183.80722379684448, -420.15775179862976, -393.40277194976807, -428.0646233558655, -434.21249663829803, -519.7159857749939, -375.48286151885986, -260.8286500573158, -330.29503148794174, -323.56794291734695, -347.4284827709198, -198.13383594155312, -377.22158646583557, -244.5910194516182, -402.7679386138916, -420.6286367177963, -396.242295563221, -450.289701461792, -378.6871826648712, -302.7411494255066, -264.4572063088417, -229.5911789238453, -411.12896835803986, -255.32924729585648, -412.1517572402954, -437.4641658067703, -354.8267130255699, -410.74869310855865, -250.73051500320435, -376.4023779630661, -455.8794755935669, -292.6408601999283, -291.09790647029877, -231.431975543499, -170.59377431869507, -438.83706909418106, -245.5051901936531, -201.12429708242416, -248.63457864522934, -411.2334289550781, -441.72914147377014, -394.88311219215393, -448.6046099662781, -240.58134478330612, -347.93346869945526, -364.09698498249054, -315.6426248550415, -262.2605593800545, -423.9374896287918, -439.4690316915512, -334.32286381721497, -378.72508811950684, -282.9626198410988, -284.43811643123627], 'episode_lengths': [51, 200, 200, 200, 149, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 11, 200, 200, 1, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], 'policy_default_policy_reward': [-15.698133781552315, -153.3492911607027, -450.6548002958298, -390.75946855545044, -62.55089320987463, -191.14457085728645, -389.36784744262695, -309.03702062368393, -318.5986790060997, -359.6790996789932, -238.84544336795807, -394.66781079769135, -381.6682622432709, -287.2933521270752, -217.4937076717615, -179.0432384610176, -380.0932892560959, -402.2531422972679, -409.27451610565186, -343.84083300828934, -364.81315791606903, -271.6467836499214, 6.77240813523531, -420.44991636276245, -322.57012808322906, 9.928175829350948, -296.3358725309372, -229.9737567305565, -312.4900425672531, -471.0608569383621, -358.43216490745544, -338.26797235012054, -287.8806974887848, -432.964985370636, -153.07981809973717, -459.13139259815216, -297.03564727306366, -253.60449689626694, -349.8328113555908, -370.17685198783875, -408.9625543355942, -267.2017743587494, -401.6510409116745, -348.58081609010696, -179.7777094244957, -314.5466828942299, -183.80722379684448, -420.15775179862976, -393.40277194976807, -428.0646233558655, -434.21249663829803, -519.7159857749939, -375.48286151885986, -260.8286500573158, -330.29503148794174, -323.56794291734695, -347.4284827709198, -198.13383594155312, -377.22158646583557, -244.5910194516182, -402.7679386138916, -420.6286367177963, -396.242295563221, -450.289701461792, -378.6871826648712, -302.7411494255066, -264.4572063088417, -229.5911789238453, -411.12896835803986, -255.32924729585648, -412.1517572402954, -437.4641658067703, -354.8267130255699, -410.74869310855865, -250.73051500320435, -376.4023779630661, -455.8794755935669, -292.6408601999283, -291.09790647029877, -231.431975543499, -170.59377431869507, -438.83706909418106, -245.5051901936531, -201.12429708242416, -248.63457864522934, -411.2334289550781, -441.72914147377014, -394.88311219215393, -448.6046099662781, -240.58134478330612, -347.93346869945526, -364.09698498249054, -315.6426248550415, -262.2605593800545, -423.9374896287918, -439.4690316915512, -334.32286381721497, -378.72508811950684, -282.9626198410988, -284.43811643123627]}, 'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.3985521673731111), 'mean_inference_ms': np.float64(0.6796884589790813), 'mean_action_processing_ms': np.float64(0.1658733200903266), 'mean_env_wait_ms': np.float64(0.22351680841738233), 'mean_env_render_ms': np.float64(0.0)}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.001623392105102539), 'StateBufferConnector_ms': np.float64(0.001436471939086914), 'ViewRequirementAgentConnector_ms': np.float64(0.032305002212524414)}, 'num_episodes': 10, 'episode_return_max': 9.928175829350948, 'episode_return_min': -519.7159857749939, 'episode_return_mean': np.float64(-323.06744350470603), 'episodes_this_iter': 10}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 82000, 'num_agent_steps_trained': 82000, 'num_env_steps_sampled': 82000, 'num_env_steps_trained': 82000, 'num_env_steps_sampled_this_iter': 2000, 'num_env_steps_trained_this_iter': 2000, 'num_env_steps_sampled_throughput_per_sec': 334.15556945335186, 'num_env_steps_trained_throughput_per_sec': 334.15556945335186, 'timesteps_total': 82000, 'num_env_steps_sampled_lifetime': 82000, 'num_agent_steps_sampled_lifetime': 82000, 'num_steps_trained_this_iter': 2000, 'agent_timesteps_total': 82000, 'timers': {'training_iteration_time_ms': 5882.706, 'restore_workers_time_ms': 0.01, 'training_step_time_ms': 5882.673, 'sample_time_ms': 732.672, 'load_time_ms': 0.298, 'load_throughput': 6717872.988, 'learn_time_ms': 5147.167, 'learn_throughput': 388.563, 'synch_weights_time_ms': 2.385}, 'counters': {'num_env_steps_sampled': 82000, 'num_env_steps_trained': 82000, 'num_agent_steps_sampled': 82000, 'num_agent_steps_trained': 82000}, 'done': False, 'training_iteration': 41, 'trial_id': 'default', 'date': '2025-11-29_19-48-56', 'timestamp': 1764463736, 'time_this_iter_s': 5.990345001220703, 'time_total_s': 243.56209182739258, 'pid': 87256, 'hostname': 'mac.lan', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': <class '__main__.FrogFly3DEnv'>, 'env_config': {'space_size': 1.0, 'step_size': 0.1, 'fly_speed': 0.05, 'catch_radius': 0.15, 'max_steps': 200, 'num_freqs': 4, 'use_positional_encodings': True}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 1, 'create_local_env_runner': True, 'num_envs_per_env_runner': 4, 'gym_env_vectorize_mode': 'sync', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 0.0001, 'grad_clip': 0.5, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 2000, 'num_epochs': 10, 'minibatch_size': 256, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'frog_transformer_policy', 'custom_model_config': {'d_model': 64, 'nhead': 4, 'num_layers': 1, 'dim_feedforward': 128, 'dropout': 0.1}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x17eca1750>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': False, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': True, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 243.56209182739258, 'iterations_since_restore': 41, 'perf': {'cpu_util_percent': np.float64(26.9625), 'ram_util_percent': np.float64(81.2875)}})\n",
      "[Iter   31] train_reward= -325.82, train_len= 194.2\n",
      "[Iter   32] train_reward= -322.87, train_len= 192.2\n",
      "[Iter   33] train_reward= -330.28, train_len= 196.1\n",
      "[Iter   34] train_reward= -330.23, train_len= 196.1\n",
      "[Iter   35] train_reward= -328.75, train_len= 196.1\n",
      "[Iter   36] train_reward= -328.12, train_len= 196.1\n",
      "[Iter   37] train_reward= -326.25, train_len= 195.8\n",
      "[Iter   38] train_reward= -331.74, train_len= 195.8\n",
      "[Iter   39] train_reward= -329.78, train_len= 195.8\n",
      "[Iter   40] train_reward= -325.74, train_len= 196.0, eval_return= -293.19 Â± 106.37, eval_len= 198.0\n",
      "[Iter   41] train_reward= -326.20, train_len= 196.0\n",
      "[Iter   42] train_reward= -329.41, train_len= 198.0\n",
      "[Iter   43] train_reward= -324.42, train_len= 198.0\n",
      "[Iter   44] train_reward= -326.11, train_len= 198.0\n",
      "[Iter   45] train_reward= -322.26, train_len= 198.0\n",
      "[Iter   46] train_reward= -320.95, train_len= 198.0\n",
      "[Iter   47] train_reward= -318.23, train_len= 198.3\n",
      "[Iter   48] train_reward= -315.22, train_len= 198.3\n",
      "[Iter   49] train_reward= -316.88, train_len= 196.8\n",
      "[Iter   50] train_reward= -325.14, train_len= 198.4, eval_return= -356.52 Â±  55.24, eval_len= 200.0\n",
      "[Iter   51] train_reward= -329.57, train_len= 196.8\n",
      "[Iter   52] train_reward= -328.72, train_len= 196.8\n",
      "[Iter   53] train_reward= -331.04, train_len= 196.8\n",
      "[Iter   54] train_reward= -331.11, train_len= 196.8\n",
      "[Iter   55] train_reward= -330.94, train_len= 196.8\n",
      "[Iter   56] train_reward= -333.41, train_len= 196.8\n",
      "[Iter   57] train_reward= -329.64, train_len= 193.0\n",
      "[Iter   58] train_reward= -322.22, train_len= 191.3\n",
      "[Iter   59] train_reward= -319.78, train_len= 192.9\n",
      "[Iter   60] train_reward= -310.30, train_len= 193.0, eval_return= -403.20 Â±  53.05, eval_len= 200.0\n",
      "[Iter   61] train_reward= -311.95, train_len= 193.0\n",
      "[Iter   62] train_reward= -312.33, train_len= 193.0\n",
      "[Iter   63] train_reward= -309.42, train_len= 193.0\n",
      "[Iter   64] train_reward= -308.69, train_len= 191.1\n",
      "[Iter   65] train_reward= -309.88, train_len= 191.1\n",
      "[Iter   66] train_reward= -310.27, train_len= 193.1\n",
      "[Iter   67] train_reward= -312.29, train_len= 194.8\n",
      "[Iter   68] train_reward= -316.60, train_len= 196.6\n",
      "[Iter   69] train_reward= -322.56, train_len= 198.1\n",
      "[Iter   70] train_reward= -324.15, train_len= 198.1, eval_return= -281.03 Â±  49.82, eval_len= 200.0\n",
      "[Iter   71] train_reward= -326.33, train_len= 198.1\n",
      "[Iter   72] train_reward= -321.28, train_len= 196.1\n",
      "[Iter   73] train_reward= -328.46, train_len= 196.1\n",
      "[Iter   74] train_reward= -315.52, train_len= 196.1\n",
      "[Iter   75] train_reward= -312.21, train_len= 196.1\n",
      "[Iter   76] train_reward= -315.95, train_len= 196.1\n",
      "[Iter   77] train_reward= -313.10, train_len= 196.1\n",
      "[Iter   78] train_reward= -318.82, train_len= 196.1\n",
      "[Iter   79] train_reward= -308.92, train_len= 196.1\n",
      "[Iter   80] train_reward= -308.74, train_len= 196.1, eval_return= -324.73 Â±  58.53, eval_len= 200.0\n",
      "[Iter   81] train_reward= -305.37, train_len= 196.1\n",
      "[Iter   82] train_reward= -307.75, train_len= 198.1\n",
      "[Iter   83] train_reward= -308.17, train_len= 200.0\n",
      "[Iter   84] train_reward= -313.23, train_len= 200.0\n",
      "[Iter   85] train_reward= -318.42, train_len= 200.0\n",
      "[Iter   86] train_reward= -323.80, train_len= 200.0\n",
      "[Iter   87] train_reward= -320.40, train_len= 200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(checkpoint_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# ---- 1. One PPO training iteration ----\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     train_reward, train_len \u001b[38;5;241m=\u001b[39m get_train_metrics(result)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# ---- 2. Periodic evaluation ----\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:1245\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration()\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1242\u001b[0m         (\n\u001b[1;32m   1243\u001b[0m             train_results,\n\u001b[1;32m   1244\u001b[0m             train_iter_ctx,\n\u001b[0;32m-> 1245\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mevaluation_parallel_to_training:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:4330\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration_old_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_on_env_runners_recreated_callbacks(\n\u001b[1;32m   4324\u001b[0m             config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   4325\u001b[0m             env_runner_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group,\n\u001b[1;32m   4326\u001b[0m             restored_env_runner_indices\u001b[38;5;241m=\u001b[39mrestored,\n\u001b[1;32m   4327\u001b[0m         )\n\u001b[1;32m   4329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_STEP_TIMER]:\n\u001b[0;32m-> 4330\u001b[0m     training_step_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_results:\n\u001b[1;32m   4333\u001b[0m     results \u001b[38;5;241m=\u001b[39m training_step_results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py:393\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;129m@override\u001b[39m(Algorithm)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# Old API stack (Policy, RolloutWorker, Connector).\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_step_old_api_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# Collect batches from sample workers until we have a full batch.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, ENV_RUNNER_SAMPLING_TIMER)):\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;66;03m# Sample in parallel from the workers.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo.py:489\u001b[0m, in \u001b[0;36mPPO._training_step_old_api_stack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m policies_to_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(train_results\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    493\u001b[0m global_vars \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED],\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# TODO (sven): num_grad_updates per each policy should be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m     },\n\u001b[1;32m    501\u001b[0m }\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/execution/train_ops.py:181\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[0;34m(algorithm, train_batch)\u001b[0m\n\u001b[1;32m    176\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/policy/torch_policy_v2.py:746\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_loaded_batch\u001b[0;34m(self, offset, buffer_index)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    744\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_batches[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][offset : offset \u001b[38;5;241m+\u001b[39m device_batch_size]\n\u001b[0;32m--> 746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;66;03m# Copy weights of main model (tower-0) to all other towers.\u001b[39;00m\n\u001b[1;32m    750\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/policy/torch_policy_v2.py:627\u001b[0m, in \u001b[0;36mTorchPolicyV2.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    622\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[1;32m    623\u001b[0m )\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# Compute gradients (will calculate all losses and `backward()`\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# them to get the grads).\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m grads, fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# Step the optimizers.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(_directStepOptimizerSingleton)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/utils/threading.py:24\u001b[0m, in \u001b[0;36mwith_lock.<locals>.wrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_lock\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/policy/torch_policy_v2.py:835\u001b[0m, in \u001b[0;36mTorchPolicyV2.compute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_tensor_dict(postprocessed_batch, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Do the (maybe parallelized) gradient calculation step.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m tower_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_gpu_parallel_grad_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpostprocessed_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m all_grads, grad_info \u001b[38;5;241m=\u001b[39m tower_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    839\u001b[0m grad_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallreduce_latency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizers)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/policy/torch_policy_v2.py:1232\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc\u001b[0;34m(self, sample_batches)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fake_gpus\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard_idx, (model, sample_batch, device) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m   1230\u001b[0m         \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_gpu_towers, sample_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices)\n\u001b[1;32m   1231\u001b[0m     ):\n\u001b[0;32m-> 1232\u001b[0m         \u001b[43m_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;66;03m# Raise errors right away for better debugging.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m         last_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/policy/torch_policy_v2.py:1151\u001b[0m, in \u001b[0;36mTorchPolicyV2._multi_gpu_parallel_grad_calc.<locals>._worker\u001b[0;34m(shard_idx, model, sample_batch, device)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m NullContextManager() \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m         device\n\u001b[1;32m   1149\u001b[0m     ):\n\u001b[1;32m   1150\u001b[0m         loss_out \u001b[38;5;241m=\u001b[39m force_list(\n\u001b[0;32m-> 1151\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m         )\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;66;03m# Call Model's custom-loss with Policy loss outputs and\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;66;03m# train_batch.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo_torch_policy.py:84\u001b[0m, in \u001b[0;36mPPOTorchPolicy.loss\u001b[0;34m(self, model, dist_class, train_batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;129m@override\u001b[39m(TorchPolicyV2)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     train_batch: SampleBatch,\n\u001b[1;32m     72\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TensorType, List[TensorType]]:\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute loss for Proximal Policy Objective.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m        The PPO loss tensor given the input batch.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     logits, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     curr_action_dist \u001b[38;5;241m=\u001b[39m dist_class(logits, model)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# RNN case: Mask away 0-padded chunks at end of time axis.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/ray/rllib/models/modelv2.py:257\u001b[0m, in \u001b[0;36mModelV2.__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    254\u001b[0m         restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext():\n\u001b[0;32m--> 257\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_dict, SampleBatch):\n\u001b[1;32m    260\u001b[0m     input_dict\u001b[38;5;241m.\u001b[39maccessed_keys \u001b[38;5;241m=\u001b[39m restored\u001b[38;5;241m.\u001b[39maccessed_keys \u001b[38;5;241m-\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_flat\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m, in \u001b[0;36mFrogTransformerModel.forward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(x)                    \u001b[38;5;66;03m# (B, T, d_model)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# (B, T, d_model)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Mean-pool tokens\u001b[39;00m\n\u001b[1;32m     80\u001b[0m features \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)             \u001b[38;5;66;03m# (B, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/transformer.py:935\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    931\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    934\u001b[0m         x\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m     )\n\u001b[1;32m    937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/transformer.py:958\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    944\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    948\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    949\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    950\u001b[0m         x,\n\u001b[1;32m    951\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    957\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rl-project/lib/python3.10/site-packages/torch/nn/functional.py:1418\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1418\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1419\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "num_iterations = 200        # ðŸ” how long you want to train\n",
    "eval_interval  = 10         # ðŸ“ˆ how often to run eval\n",
    "eval_episodes  = 5\n",
    "\n",
    "train_history = []          # for plotting later\n",
    "best_eval_return = -float(\"inf\")\n",
    "checkpoint_dir = \"frog_transformer_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, num_iterations + 1):\n",
    "    # ---- 1. One PPO training iteration ----\n",
    "    result = algo.train()\n",
    "    train_reward, train_len = get_train_metrics(result)\n",
    "\n",
    "    # ---- 2. Periodic evaluation ----\n",
    "    if i % eval_interval == 0:\n",
    "        eval_stats = evaluate_policy(algo, eval_env, num_episodes=eval_episodes, render=False)\n",
    "        eval_return = eval_stats[\"mean_return\"]\n",
    "        eval_len    = eval_stats[\"mean_length\"]\n",
    "\n",
    "        # Log nicely\n",
    "        print(\n",
    "            f\"[Iter {i:4d}] \"\n",
    "            f\"train_reward={train_reward:8.2f}, \"\n",
    "            f\"train_len={train_len:6.1f}, \"\n",
    "            f\"eval_return={eval_return:8.2f} Â± {eval_stats['std_return']:6.2f}, \"\n",
    "            f\"eval_len={eval_len:6.1f}\"\n",
    "        )\n",
    "\n",
    "        # Save history\n",
    "        train_history.append(\n",
    "            {\n",
    "                \"iter\": i,\n",
    "                \"train_reward\": train_reward,\n",
    "                \"train_len\": train_len,\n",
    "                \"eval_return\": eval_return,\n",
    "                \"eval_len\": eval_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ---- 3. Optional: save best checkpoint ----\n",
    "        if eval_return > best_eval_return:\n",
    "            best_eval_return = eval_return\n",
    "            checkpoint_path = algo.save(checkpoint_dir)\n",
    "            print(f\"  âœ… New best eval_return={best_eval_return:.2f}, saved to {checkpoint_path}\")\n",
    "    else:\n",
    "        # Lighter log on non-eval iterations\n",
    "        print(\n",
    "            f\"[Iter {i:4d}] \"\n",
    "            f\"train_reward={train_reward:8.2f}, \"\n",
    "            f\"train_len={train_len:6.1f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_hist = pd.DataFrame(train_history)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_hist[\"iter\"], df_hist[\"train_reward\"], label=\"train reward\")\n",
    "plt.plot(df_hist[\"iter\"], df_hist[\"eval_return\"], label=\"eval return\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Episode return\")\n",
    "plt.legend()\n",
    "plt.title(\"Frog Transformer PPO training\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
